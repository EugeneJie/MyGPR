{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "毕设（异常定位训练）.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl1QVV3pIq2F",
        "colab_type": "code",
        "outputId": "a61eccfd-cd4f-4a44-8954-498ba6e37a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dh7kknGJUF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/gdrive/My Drive/gpr_locate\"\n",
        "os.chdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVKte-TUcvud",
        "colab_type": "code",
        "outputId": "55d5677f-09b0-4f8f-a7d4-c848c7e4b683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/gpr_locate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_bssR2eIslm",
        "colab_type": "code",
        "outputId": "30a8867b-acf8-4e28-f151-cc190588b1b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12727
        }
      },
      "source": [
        "\"\"\"\n",
        "Retrain the YOLO model for your own dataset.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Lambda\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
        "from yolo3.utils import get_random_data\n",
        "\n",
        "\n",
        "def _main():\n",
        "    annotation_path = 'model_data/train.txt'\n",
        "    log_dir = 'model_data/logs/'\n",
        "    classes_path = 'model_data/my_classes.txt'\n",
        "    anchors_path = 'model_data/yolo_anchors.txt'\n",
        "    class_names = get_classes(classes_path)\n",
        "    num_classes = len(class_names)\n",
        "    anchors = get_anchors(anchors_path)\n",
        "\n",
        "    input_shape = (416,416) # multiple of 32, hw\n",
        "\n",
        "    is_tiny_version = len(anchors)==6 # default setting\n",
        "    if is_tiny_version:\n",
        "        model = create_tiny_model(input_shape, anchors, num_classes,\n",
        "            freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')\n",
        "    else:\n",
        "        model = create_model(input_shape, anchors, num_classes,\n",
        "            freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze\n",
        "\n",
        "    logging = TensorBoard(log_dir=log_dir)\n",
        "    '''\n",
        "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
        "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
        "    '''  \n",
        "    checkpoint = ModelCheckpoint(log_dir + 'trained_weights.h5',\n",
        "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "\n",
        "    val_split = 0.1\n",
        "    with open(annotation_path) as f:\n",
        "        lines = f.readlines()\n",
        "    np.random.seed(10101)\n",
        "    np.random.shuffle(lines)\n",
        "    np.random.seed(None)\n",
        "    num_val = int(len(lines)*val_split)\n",
        "    num_train = len(lines) - num_val\n",
        "\n",
        "    # Train with frozen layers first, to get a stable loss.\n",
        "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
        "    if False:\n",
        "        model.compile(optimizer=Adam(lr=1e-3), loss={\n",
        "            # use custom yolo_loss Lambda layer.\n",
        "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
        "\n",
        "        batch_size = 8\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "                steps_per_epoch=max(1, num_train//batch_size),\n",
        "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "                validation_steps=max(1, num_val//batch_size),\n",
        "                epochs=300,\n",
        "                initial_epoch=0,\n",
        "                callbacks=[logging, checkpoint])\n",
        "        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
        "\n",
        "    # Unfreeze and continue training, to fine-tune.\n",
        "    # Train longer if the result is not good.\n",
        "    if True:\n",
        "        for i in range(len(model.layers)):\n",
        "            model.layers[i].trainable = True\n",
        "        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
        "        print('Unfreeze all of the layers.')\n",
        "\n",
        "        batch_size = 8 # note that more GPU memory is required after unfreezing the body\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "            steps_per_epoch=max(1, num_train//batch_size),\n",
        "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "            validation_steps=max(1, num_val//batch_size),\n",
        "            epochs=600,\n",
        "            initial_epoch=300,\n",
        "            #callbacks=[logging, checkpoint, reduce_lr, early_stopping]\n",
        "            callbacks=[logging, checkpoint, reduce_lr])\n",
        "        model.save_weights(log_dir + 'trained_weights_final.h5')\n",
        "\n",
        "    # Further training if needed.\n",
        "\n",
        "\n",
        "def get_classes(classes_path):\n",
        "    '''loads the classes'''\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "def get_anchors(anchors_path):\n",
        "    '''loads the anchors from a file'''\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "    anchors = [float(x) for x in anchors.split(',')]\n",
        "    return np.array(anchors).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='model_data/yolo_weights.h5'):\n",
        "    '''create the training model'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
        "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
        "\n",
        "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
        "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
        "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
        "    '''create the training model, for Tiny YOLOv3'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
        "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
        "\n",
        "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
        "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze the darknet body or freeze all but 2 output layers.\n",
        "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    '''data generator for fit_generator'''\n",
        "    n = len(annotation_lines)\n",
        "    i = 0\n",
        "    while True:\n",
        "        image_data = []\n",
        "        box_data = []\n",
        "        for b in range(batch_size):\n",
        "            if i==0:\n",
        "                np.random.shuffle(annotation_lines)\n",
        "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
        "            image_data.append(image)\n",
        "            box_data.append(box)\n",
        "            i = (i+1) % n\n",
        "        image_data = np.array(image_data)\n",
        "        box_data = np.array(box_data)\n",
        "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
        "        yield [image_data, *y_true], np.zeros(batch_size)\n",
        "\n",
        "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    n = len(annotation_lines)\n",
        "    if n==0 or batch_size<=0: return None\n",
        "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    _main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Create YOLOv3 model with 9 anchors and 1 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((1, 1, 1024, 18) vs (255, 1024, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((18,) vs (255,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((1, 1, 512, 18) vs (255, 512, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((18,) vs (255,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((1, 1, 256, 18) vs (255, 256, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((18,) vs (255,)).\n",
            "  weight_values[i].shape))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load weights model_data/yolo_weights.h5.\n",
            "Freeze the first 249 layers of total 252 layers.\n",
            "Unfreeze all of the layers.\n",
            "Train on 180 samples, val on 20 samples, with batch size 8.\n",
            "Epoch 301/600\n",
            "22/22 [==============================] - 106s 5s/step - loss: 7380.7952 - val_loss: 3906.9021\n",
            "Epoch 302/600\n",
            "22/22 [==============================] - 45s 2s/step - loss: 3158.0627 - val_loss: 4222.9543\n",
            "Epoch 303/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 1875.0400 - val_loss: 1392.8790\n",
            "Epoch 304/600\n",
            "22/22 [==============================] - 32s 1s/step - loss: 1388.3951 - val_loss: 1137.0671\n",
            "Epoch 305/600\n",
            "22/22 [==============================] - 44s 2s/step - loss: 1118.1970 - val_loss: 1014.3470\n",
            "Epoch 306/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 962.5568 - val_loss: 1108.0232\n",
            "Epoch 307/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 829.7453 - val_loss: 877.8204\n",
            "Epoch 308/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 683.3863 - val_loss: 672.9468\n",
            "Epoch 309/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 594.6501 - val_loss: 688.6651\n",
            "Epoch 310/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 527.2158 - val_loss: 534.6638\n",
            "Epoch 311/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 467.9857 - val_loss: 464.2006\n",
            "Epoch 312/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 419.6379 - val_loss: 444.9505\n",
            "Epoch 313/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 378.8102 - val_loss: 412.6977\n",
            "Epoch 314/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 346.7751 - val_loss: 363.7053\n",
            "Epoch 315/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 317.3554 - val_loss: 443.8571\n",
            "Epoch 316/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 290.4686 - val_loss: 318.9888\n",
            "Epoch 317/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 267.7675 - val_loss: 287.2034\n",
            "Epoch 318/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 247.5250 - val_loss: 261.7217\n",
            "Epoch 319/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 229.5730 - val_loss: 244.2246\n",
            "Epoch 320/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 214.2228 - val_loss: 221.4759\n",
            "Epoch 321/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 200.7121 - val_loss: 209.1721\n",
            "Epoch 322/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 187.8502 - val_loss: 196.0229\n",
            "Epoch 323/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 176.4253 - val_loss: 183.8890\n",
            "Epoch 324/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 166.6663 - val_loss: 172.5581\n",
            "Epoch 325/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 157.8908 - val_loss: 162.2118\n",
            "Epoch 326/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 149.3026 - val_loss: 154.0189\n",
            "Epoch 327/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 141.4589 - val_loss: 148.2623\n",
            "Epoch 328/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 134.0778 - val_loss: 139.5640\n",
            "Epoch 329/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 127.5823 - val_loss: 131.2821\n",
            "Epoch 330/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 121.5461 - val_loss: 124.7937\n",
            "Epoch 331/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 115.5464 - val_loss: 118.9057\n",
            "Epoch 332/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 110.6376 - val_loss: 118.7957\n",
            "Epoch 333/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 106.2661 - val_loss: 108.5958\n",
            "Epoch 334/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 103.4292 - val_loss: 103.4266\n",
            "Epoch 335/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 97.9726 - val_loss: 100.1732\n",
            "Epoch 336/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 93.1565 - val_loss: 95.9347\n",
            "Epoch 337/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 89.5643 - val_loss: 92.6459\n",
            "Epoch 338/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 86.6234 - val_loss: 90.0493\n",
            "Epoch 339/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 83.3267 - val_loss: 85.5248\n",
            "Epoch 340/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 80.4827 - val_loss: 82.5265\n",
            "Epoch 341/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 77.7480 - val_loss: 78.8224\n",
            "Epoch 342/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 74.8527 - val_loss: 77.1986\n",
            "Epoch 343/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 72.7277 - val_loss: 74.7591\n",
            "Epoch 344/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 70.0820 - val_loss: 72.0045\n",
            "Epoch 345/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 68.1559 - val_loss: 69.4505\n",
            "Epoch 346/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 65.9974 - val_loss: 67.9481\n",
            "Epoch 347/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 64.1025 - val_loss: 65.0169\n",
            "Epoch 348/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 62.4886 - val_loss: 64.0616\n",
            "Epoch 349/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 60.8011 - val_loss: 61.3729\n",
            "Epoch 350/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 59.0365 - val_loss: 61.0186\n",
            "Epoch 351/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 57.3837 - val_loss: 60.0010\n",
            "Epoch 352/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 55.8135 - val_loss: 57.2864\n",
            "Epoch 353/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 54.4772 - val_loss: 56.5201\n",
            "Epoch 354/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 53.0662 - val_loss: 57.6837\n",
            "Epoch 355/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 51.7294 - val_loss: 55.0930\n",
            "Epoch 356/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 50.4611 - val_loss: 57.1972\n",
            "Epoch 357/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 49.3145 - val_loss: 50.4818\n",
            "Epoch 358/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 48.2440 - val_loss: 50.6326\n",
            "Epoch 359/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 47.3317 - val_loss: 48.1672\n",
            "Epoch 360/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 45.9578 - val_loss: 46.4827\n",
            "Epoch 361/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 45.1767 - val_loss: 46.1555\n",
            "Epoch 362/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 44.1306 - val_loss: 44.6537\n",
            "Epoch 363/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 43.1929 - val_loss: 44.1587\n",
            "Epoch 364/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 42.1920 - val_loss: 42.4949\n",
            "Epoch 365/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 41.5398 - val_loss: 42.1915\n",
            "Epoch 366/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 40.7175 - val_loss: 41.0161\n",
            "Epoch 367/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 39.9969 - val_loss: 40.6480\n",
            "Epoch 368/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 39.2500 - val_loss: 39.9292\n",
            "Epoch 369/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 38.4148 - val_loss: 41.1713\n",
            "Epoch 370/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 37.8098 - val_loss: 42.0708\n",
            "Epoch 371/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 37.1863 - val_loss: 39.4046\n",
            "Epoch 372/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 36.4367 - val_loss: 37.6047\n",
            "Epoch 373/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 35.9610 - val_loss: 36.2122\n",
            "Epoch 374/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 35.3582 - val_loss: 35.2141\n",
            "Epoch 375/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 34.7368 - val_loss: 35.4366\n",
            "Epoch 376/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 34.2007 - val_loss: 34.7777\n",
            "Epoch 377/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 33.5779 - val_loss: 33.8707\n",
            "Epoch 378/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 33.0320 - val_loss: 33.2188\n",
            "Epoch 379/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 32.5144 - val_loss: 32.5764\n",
            "Epoch 380/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 32.0430 - val_loss: 31.7993\n",
            "Epoch 381/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 31.6357 - val_loss: 31.0559\n",
            "Epoch 382/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 31.0014 - val_loss: 31.0290\n",
            "Epoch 383/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 30.5602 - val_loss: 30.4420\n",
            "Epoch 384/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 30.2707 - val_loss: 30.2528\n",
            "Epoch 385/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 29.7044 - val_loss: 29.4632\n",
            "Epoch 386/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 29.5155 - val_loss: 29.8908\n",
            "Epoch 387/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 28.9256 - val_loss: 28.9500\n",
            "Epoch 388/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 28.5747 - val_loss: 29.4498\n",
            "Epoch 389/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 28.2919 - val_loss: 28.1972\n",
            "Epoch 390/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 27.7065 - val_loss: 28.6304\n",
            "Epoch 391/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 27.3154 - val_loss: 27.3068\n",
            "Epoch 392/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 27.0391 - val_loss: 27.1212\n",
            "Epoch 393/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 26.7095 - val_loss: 27.1368\n",
            "Epoch 394/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 26.4004 - val_loss: 26.4098\n",
            "Epoch 395/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 26.1199 - val_loss: 26.5738\n",
            "Epoch 396/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 25.7631 - val_loss: 25.6336\n",
            "Epoch 397/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 25.4382 - val_loss: 25.6418\n",
            "Epoch 398/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 25.0963 - val_loss: 25.3599\n",
            "Epoch 399/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 24.9398 - val_loss: 25.0402\n",
            "Epoch 400/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 24.8106 - val_loss: 24.8154\n",
            "Epoch 401/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 24.4703 - val_loss: 23.5409\n",
            "Epoch 402/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 24.2540 - val_loss: 24.0384\n",
            "Epoch 403/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 24.0331 - val_loss: 23.8445\n",
            "Epoch 404/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.7250 - val_loss: 23.7217\n",
            "\n",
            "Epoch 00404: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "Epoch 405/600\n",
            "22/22 [==============================] - 44s 2s/step - loss: 23.4988 - val_loss: 24.8981\n",
            "Epoch 406/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.4570 - val_loss: 23.9708\n",
            "Epoch 407/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.4944 - val_loss: 23.4223\n",
            "Epoch 408/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3379 - val_loss: 24.0222\n",
            "Epoch 409/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.5656 - val_loss: 23.3395\n",
            "Epoch 410/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.4101 - val_loss: 23.2692\n",
            "Epoch 411/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2910 - val_loss: 23.3431\n",
            "Epoch 412/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2603 - val_loss: 23.5372\n",
            "Epoch 413/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2477 - val_loss: 23.4897\n",
            "\n",
            "Epoch 00413: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "Epoch 414/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2188 - val_loss: 23.2083\n",
            "Epoch 415/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2341 - val_loss: 23.2138\n",
            "Epoch 416/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.3219 - val_loss: 23.5127\n",
            "Epoch 417/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1580 - val_loss: 23.1631\n",
            "Epoch 418/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2891 - val_loss: 23.2189\n",
            "Epoch 419/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1947 - val_loss: 23.0425\n",
            "Epoch 420/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.3034 - val_loss: 23.1673\n",
            "Epoch 421/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1871 - val_loss: 23.4197\n",
            "Epoch 422/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2839 - val_loss: 23.0884\n",
            "\n",
            "Epoch 00422: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "Epoch 423/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1384 - val_loss: 23.0913\n",
            "Epoch 424/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2605 - val_loss: 22.9719\n",
            "Epoch 425/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.3085 - val_loss: 23.4167\n",
            "Epoch 426/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1697 - val_loss: 22.9586\n",
            "Epoch 427/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2817 - val_loss: 23.5152\n",
            "Epoch 428/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2562 - val_loss: 23.1440\n",
            "Epoch 429/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1758 - val_loss: 22.9527\n",
            "Epoch 430/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2326 - val_loss: 23.1909\n",
            "Epoch 431/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.1384 - val_loss: 23.1618\n",
            "Epoch 432/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2592 - val_loss: 23.0902\n",
            "\n",
            "Epoch 00432: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "Epoch 433/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3207 - val_loss: 23.2202\n",
            "Epoch 434/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2099 - val_loss: 22.9225\n",
            "Epoch 435/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2298 - val_loss: 23.1291\n",
            "Epoch 436/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2839 - val_loss: 23.2693\n",
            "Epoch 437/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2437 - val_loss: 23.1855\n",
            "\n",
            "Epoch 00437: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
            "Epoch 438/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1248 - val_loss: 23.3200\n",
            "Epoch 439/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1728 - val_loss: 23.2658\n",
            "Epoch 440/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2586 - val_loss: 23.4082\n",
            "\n",
            "Epoch 00440: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
            "Epoch 441/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2378 - val_loss: 23.3883\n",
            "Epoch 442/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1870 - val_loss: 23.1334\n",
            "Epoch 443/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2877 - val_loss: 23.1760\n",
            "\n",
            "Epoch 00443: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
            "Epoch 444/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.3685 - val_loss: 23.1158\n",
            "Epoch 445/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1658 - val_loss: 23.0254\n",
            "Epoch 446/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3217 - val_loss: 23.0830\n",
            "\n",
            "Epoch 00446: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
            "Epoch 447/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3015 - val_loss: 23.0381\n",
            "Epoch 448/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2085 - val_loss: 23.1634\n",
            "Epoch 449/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2552 - val_loss: 23.4166\n",
            "\n",
            "Epoch 00449: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
            "Epoch 450/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1888 - val_loss: 23.2023\n",
            "Epoch 451/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1498 - val_loss: 22.9066\n",
            "Epoch 452/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3165 - val_loss: 23.0753\n",
            "Epoch 453/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2688 - val_loss: 23.0731\n",
            "Epoch 454/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.1926 - val_loss: 23.1342\n",
            "\n",
            "Epoch 00454: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
            "Epoch 455/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2582 - val_loss: 23.0823\n",
            "Epoch 456/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2230 - val_loss: 22.9135\n",
            "Epoch 457/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2632 - val_loss: 23.0160\n",
            "\n",
            "Epoch 00457: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
            "Epoch 458/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2438 - val_loss: 23.0141\n",
            "Epoch 459/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1894 - val_loss: 23.2151\n",
            "Epoch 460/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2313 - val_loss: 23.4364\n",
            "\n",
            "Epoch 00460: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
            "Epoch 461/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2510 - val_loss: 23.3243\n",
            "Epoch 462/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2141 - val_loss: 23.2774\n",
            "Epoch 463/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2941 - val_loss: 22.9969\n",
            "\n",
            "Epoch 00463: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
            "Epoch 464/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2934 - val_loss: 23.0139\n",
            "Epoch 465/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2591 - val_loss: 23.0866\n",
            "Epoch 466/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2021 - val_loss: 23.2184\n",
            "\n",
            "Epoch 00466: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
            "Epoch 467/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.1732 - val_loss: 23.3212\n",
            "Epoch 468/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2362 - val_loss: 23.0547\n",
            "Epoch 469/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2853 - val_loss: 24.1152\n",
            "\n",
            "Epoch 00469: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
            "Epoch 470/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2307 - val_loss: 23.1073\n",
            "Epoch 471/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2857 - val_loss: 23.3680\n",
            "Epoch 472/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2188 - val_loss: 23.1454\n",
            "\n",
            "Epoch 00472: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n",
            "Epoch 473/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2907 - val_loss: 23.2040\n",
            "Epoch 474/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1823 - val_loss: 23.1790\n",
            "Epoch 475/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1687 - val_loss: 22.7991\n",
            "Epoch 476/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.4070 - val_loss: 22.8045\n",
            "Epoch 477/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3304 - val_loss: 23.1438\n",
            "Epoch 478/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2304 - val_loss: 23.1720\n",
            "\n",
            "Epoch 00478: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-22.\n",
            "Epoch 479/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2139 - val_loss: 22.9240\n",
            "Epoch 480/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2383 - val_loss: 23.2976\n",
            "Epoch 481/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2493 - val_loss: 23.0671\n",
            "\n",
            "Epoch 00481: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-23.\n",
            "Epoch 482/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1823 - val_loss: 23.3200\n",
            "Epoch 483/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3128 - val_loss: 23.0963\n",
            "Epoch 484/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.4243 - val_loss: 23.1366\n",
            "\n",
            "Epoch 00484: ReduceLROnPlateau reducing learning rate to 9.999999682655227e-24.\n",
            "Epoch 485/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2189 - val_loss: 23.5952\n",
            "Epoch 486/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3125 - val_loss: 23.1327\n",
            "Epoch 487/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2987 - val_loss: 23.4964\n",
            "\n",
            "Epoch 00487: ReduceLROnPlateau reducing learning rate to 9.999999998199588e-25.\n",
            "Epoch 488/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1944 - val_loss: 23.1053\n",
            "Epoch 489/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3024 - val_loss: 23.5412\n",
            "Epoch 490/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2682 - val_loss: 22.9954\n",
            "\n",
            "Epoch 00490: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-25.\n",
            "Epoch 491/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3089 - val_loss: 23.3689\n",
            "Epoch 492/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2778 - val_loss: 23.1071\n",
            "Epoch 493/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3685 - val_loss: 23.2248\n",
            "\n",
            "Epoch 00493: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-26.\n",
            "Epoch 494/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2783 - val_loss: 23.7100\n",
            "Epoch 495/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3969 - val_loss: 23.3312\n",
            "Epoch 496/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3080 - val_loss: 23.2147\n",
            "\n",
            "Epoch 00496: ReduceLROnPlateau reducing learning rate to 9.999999887266024e-28.\n",
            "Epoch 497/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.1742 - val_loss: 23.3753\n",
            "Epoch 498/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1847 - val_loss: 23.2634\n",
            "Epoch 499/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2500 - val_loss: 23.4995\n",
            "\n",
            "Epoch 00499: ReduceLROnPlateau reducing learning rate to 1.0000000272452012e-28.\n",
            "Epoch 500/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3188 - val_loss: 22.9937\n",
            "Epoch 501/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2817 - val_loss: 22.8124\n",
            "Epoch 502/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2579 - val_loss: 23.0189\n",
            "\n",
            "Epoch 00502: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-29.\n",
            "Epoch 503/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2420 - val_loss: 23.4213\n",
            "Epoch 504/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1844 - val_loss: 23.2750\n",
            "Epoch 505/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2911 - val_loss: 23.6873\n",
            "\n",
            "Epoch 00505: ReduceLROnPlateau reducing learning rate to 1.0000000031710769e-30.\n",
            "Epoch 506/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2085 - val_loss: 23.1296\n",
            "Epoch 507/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2363 - val_loss: 23.2136\n",
            "Epoch 508/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1690 - val_loss: 23.1048\n",
            "\n",
            "Epoch 00508: ReduceLROnPlateau reducing learning rate to 1.000000003171077e-31.\n",
            "Epoch 509/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2236 - val_loss: 23.2545\n",
            "Epoch 510/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.1983 - val_loss: 23.4026\n",
            "Epoch 511/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.3141 - val_loss: 23.3853\n",
            "\n",
            "Epoch 00511: ReduceLROnPlateau reducing learning rate to 9.999999796611899e-33.\n",
            "Epoch 512/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2145 - val_loss: 23.1220\n",
            "Epoch 513/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1714 - val_loss: 23.1535\n",
            "Epoch 514/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1924 - val_loss: 23.1498\n",
            "\n",
            "Epoch 00514: ReduceLROnPlateau reducing learning rate to 9.999999502738312e-34.\n",
            "Epoch 515/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1767 - val_loss: 23.1353\n",
            "Epoch 516/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2802 - val_loss: 23.0991\n",
            "Epoch 517/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2451 - val_loss: 23.2869\n",
            "\n",
            "Epoch 00517: ReduceLROnPlateau reducing learning rate to 9.999999319067318e-35.\n",
            "Epoch 518/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3082 - val_loss: 22.9840\n",
            "Epoch 519/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2156 - val_loss: 23.2377\n",
            "Epoch 520/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3132 - val_loss: 23.3357\n",
            "\n",
            "Epoch 00520: ReduceLROnPlateau reducing learning rate to 9.999999319067319e-36.\n",
            "Epoch 521/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3338 - val_loss: 23.5451\n",
            "Epoch 522/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2745 - val_loss: 23.3582\n",
            "Epoch 523/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.4016 - val_loss: 23.1108\n",
            "\n",
            "Epoch 00523: ReduceLROnPlateau reducing learning rate to 9.999999462560281e-37.\n",
            "Epoch 524/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2861 - val_loss: 23.1525\n",
            "Epoch 525/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2444 - val_loss: 23.1430\n",
            "Epoch 526/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2582 - val_loss: 23.2238\n",
            "\n",
            "Epoch 00526: ReduceLROnPlateau reducing learning rate to 9.99999946256028e-38.\n",
            "Epoch 527/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2349 - val_loss: 23.5362\n",
            "Epoch 528/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.1977 - val_loss: 23.3642\n",
            "Epoch 529/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2688 - val_loss: 23.2021\n",
            "\n",
            "Epoch 00529: ReduceLROnPlateau reducing learning rate to 9.99999991097579e-39.\n",
            "Epoch 530/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.1079 - val_loss: 23.1767\n",
            "Epoch 531/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3652 - val_loss: 22.7619\n",
            "Epoch 532/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2425 - val_loss: 23.0194\n",
            "Epoch 533/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3153 - val_loss: 23.0306\n",
            "Epoch 534/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2916 - val_loss: 23.4179\n",
            "\n",
            "Epoch 00534: ReduceLROnPlateau reducing learning rate to 9.999999350456405e-40.\n",
            "Epoch 535/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2464 - val_loss: 23.0336\n",
            "Epoch 536/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3394 - val_loss: 23.2991\n",
            "Epoch 537/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.2183 - val_loss: 23.6666\n",
            "\n",
            "Epoch 00537: ReduceLROnPlateau reducing learning rate to 1.0000002153053334e-40.\n",
            "Epoch 538/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2499 - val_loss: 23.0841\n",
            "Epoch 539/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2672 - val_loss: 23.3026\n",
            "Epoch 540/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1339 - val_loss: 22.9527\n",
            "\n",
            "Epoch 00540: ReduceLROnPlateau reducing learning rate to 9.99994610111476e-42.\n",
            "Epoch 541/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1524 - val_loss: 23.1346\n",
            "Epoch 542/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2978 - val_loss: 23.1670\n",
            "Epoch 543/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1992 - val_loss: 23.3784\n",
            "\n",
            "Epoch 00543: ReduceLROnPlateau reducing learning rate to 9.999665841421895e-43.\n",
            "Epoch 544/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2626 - val_loss: 23.0920\n",
            "Epoch 545/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2085 - val_loss: 23.1201\n",
            "Epoch 546/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2272 - val_loss: 23.2542\n",
            "\n",
            "Epoch 00546: ReduceLROnPlateau reducing learning rate to 1.0005271035279195e-43.\n",
            "Epoch 547/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2756 - val_loss: 22.9757\n",
            "Epoch 548/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2887 - val_loss: 23.2049\n",
            "Epoch 549/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1705 - val_loss: 22.9433\n",
            "\n",
            "Epoch 00549: ReduceLROnPlateau reducing learning rate to 9.949219096706202e-45.\n",
            "Epoch 550/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1745 - val_loss: 22.9291\n",
            "Epoch 551/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2340 - val_loss: 23.7866\n",
            "Epoch 552/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.3169 - val_loss: 23.1333\n",
            "\n",
            "Epoch 00552: ReduceLROnPlateau reducing learning rate to 9.80908925027372e-46.\n",
            "Epoch 553/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2597 - val_loss: 23.6269\n",
            "Epoch 554/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1941 - val_loss: 23.0703\n",
            "Epoch 555/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2417 - val_loss: 23.6032\n",
            "\n",
            "Epoch 00555: ReduceLROnPlateau reducing learning rate to 1.4012984643248171e-46.\n",
            "Epoch 556/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2417 - val_loss: 23.6879\n",
            "Epoch 557/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3641 - val_loss: 23.2641\n",
            "Epoch 558/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1936 - val_loss: 23.0761\n",
            "Epoch 559/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2863 - val_loss: 23.4056\n",
            "Epoch 560/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2963 - val_loss: 23.3275\n",
            "Epoch 561/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2534 - val_loss: 23.0043\n",
            "Epoch 562/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2765 - val_loss: 23.1154\n",
            "Epoch 563/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2197 - val_loss: 23.1526\n",
            "Epoch 564/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1894 - val_loss: 23.1601\n",
            "Epoch 565/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2182 - val_loss: 23.0648\n",
            "Epoch 566/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2230 - val_loss: 23.1871\n",
            "Epoch 567/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2657 - val_loss: 23.1583\n",
            "Epoch 568/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3121 - val_loss: 22.8963\n",
            "Epoch 569/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2514 - val_loss: 23.4544\n",
            "Epoch 570/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2607 - val_loss: 23.1117\n",
            "Epoch 571/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1969 - val_loss: 23.0709\n",
            "Epoch 572/600\n",
            "22/22 [==============================] - 46s 2s/step - loss: 23.3402 - val_loss: 23.2449\n",
            "Epoch 573/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2630 - val_loss: 23.1294\n",
            "Epoch 574/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1406 - val_loss: 23.0602\n",
            "Epoch 575/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2546 - val_loss: 23.2770\n",
            "Epoch 576/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2616 - val_loss: 23.4275\n",
            "Epoch 577/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.3802 - val_loss: 24.1003\n",
            "Epoch 578/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.4347 - val_loss: 23.0907\n",
            "Epoch 579/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2350 - val_loss: 23.2826\n",
            "Epoch 580/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1438 - val_loss: 23.2692\n",
            "Epoch 581/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1405 - val_loss: 23.5485\n",
            "Epoch 582/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2977 - val_loss: 23.0799\n",
            "Epoch 583/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.1685 - val_loss: 23.0411\n",
            "Epoch 584/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.1676 - val_loss: 23.0730\n",
            "Epoch 585/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2182 - val_loss: 23.1895\n",
            "Epoch 586/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1904 - val_loss: 23.8210\n",
            "Epoch 587/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.1645 - val_loss: 23.4826\n",
            "Epoch 588/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2022 - val_loss: 23.3947\n",
            "Epoch 589/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2756 - val_loss: 23.0218\n",
            "Epoch 590/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1962 - val_loss: 23.3088\n",
            "Epoch 591/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2045 - val_loss: 23.4597\n",
            "Epoch 592/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.1787 - val_loss: 23.7370\n",
            "Epoch 593/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3548 - val_loss: 23.2448\n",
            "Epoch 594/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2844 - val_loss: 23.1532\n",
            "Epoch 595/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2420 - val_loss: 23.4881\n",
            "Epoch 596/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.2573 - val_loss: 23.1925\n",
            "Epoch 597/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.3127 - val_loss: 23.0253\n",
            "Epoch 598/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3248 - val_loss: 23.0755\n",
            "Epoch 599/600\n",
            "22/22 [==============================] - 48s 2s/step - loss: 23.2217 - val_loss: 23.2588\n",
            "Epoch 600/600\n",
            "22/22 [==============================] - 47s 2s/step - loss: 23.3441 - val_loss: 23.2190\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}