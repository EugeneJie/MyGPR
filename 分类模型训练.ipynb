{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5IdtyUdPng61",
    "outputId": "0c018b0c-4ffc-4e3e-98a7-20165e41c7a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDZfsOg9oWsM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/content/gdrive/My Drive\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34111
    },
    "colab_type": "code",
    "id": "sN1n96dcHV4E",
    "outputId": "23b8588d-7157-4df0-9d1c-2ff471ba1b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 4 classes.\n",
      "Found 312 images belonging to 4 classes.\n",
      "{'coal': 0, 'free_space': 1, 'pec': 2, 'soil': 3}\n",
      "未找到模型，正在开始训练......\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "19/19 [==============================] - 727s 38s/step - loss: 1.3874 - acc: 0.2380 - val_loss: 1.3860 - val_acc: 0.2564\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.38601, saving model to gpr_classify.h5\n",
      "Epoch 2/500\n",
      "19/19 [==============================] - 61s 3s/step - loss: 1.3844 - acc: 0.3151 - val_loss: 1.3757 - val_acc: 0.4071\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.38601 to 1.37569, saving model to gpr_classify.h5\n",
      "Epoch 3/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 1.3440 - acc: 0.3531 - val_loss: 1.2412 - val_acc: 0.4199\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.37569 to 1.24119, saving model to gpr_classify.h5\n",
      "Epoch 4/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 1.2113 - acc: 0.3944 - val_loss: 1.0199 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.24119 to 1.01994, saving model to gpr_classify.h5\n",
      "Epoch 5/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 1.0794 - acc: 0.4372 - val_loss: 0.9222 - val_acc: 0.5481\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01994 to 0.92222, saving model to gpr_classify.h5\n",
      "Epoch 6/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 1.0435 - acc: 0.4655 - val_loss: 0.8940 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.92222 to 0.89400, saving model to gpr_classify.h5\n",
      "Epoch 7/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.9907 - acc: 0.5052 - val_loss: 0.9085 - val_acc: 0.5769\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.89400\n",
      "Epoch 8/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.9965 - acc: 0.4805 - val_loss: 0.8736 - val_acc: 0.5705\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.89400 to 0.87355, saving model to gpr_classify.h5\n",
      "Epoch 9/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.9431 - acc: 0.5109 - val_loss: 0.8512 - val_acc: 0.5577\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.87355 to 0.85124, saving model to gpr_classify.h5\n",
      "Epoch 10/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.9273 - acc: 0.5369 - val_loss: 0.8198 - val_acc: 0.6731\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.85124 to 0.81980, saving model to gpr_classify.h5\n",
      "Epoch 11/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.8823 - acc: 0.5532 - val_loss: 0.7895 - val_acc: 0.5929\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.81980 to 0.78947, saving model to gpr_classify.h5\n",
      "Epoch 12/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.8865 - acc: 0.5496 - val_loss: 0.7171 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.78947 to 0.71706, saving model to gpr_classify.h5\n",
      "Epoch 13/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.8588 - acc: 0.5825 - val_loss: 0.8800 - val_acc: 0.5481\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.71706\n",
      "Epoch 14/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.8721 - acc: 0.5424 - val_loss: 0.7677 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.71706\n",
      "Epoch 15/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.8809 - acc: 0.5784 - val_loss: 0.7343 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.71706\n",
      "Epoch 16/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.8234 - acc: 0.6135 - val_loss: 0.7242 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.71706\n",
      "Epoch 17/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.8135 - acc: 0.5950 - val_loss: 0.6160 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.71706 to 0.61598, saving model to gpr_classify.h5\n",
      "Epoch 18/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.7214 - acc: 0.6695 - val_loss: 0.6163 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.61598\n",
      "Epoch 19/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.7192 - acc: 0.6837 - val_loss: 0.6354 - val_acc: 0.6987\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.61598\n",
      "Epoch 20/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.7247 - acc: 0.6713 - val_loss: 0.5606 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.61598 to 0.56061, saving model to gpr_classify.h5\n",
      "Epoch 21/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.7120 - acc: 0.6855 - val_loss: 0.5531 - val_acc: 0.7821\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.56061 to 0.55307, saving model to gpr_classify.h5\n",
      "Epoch 22/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.7251 - acc: 0.6727 - val_loss: 0.5843 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.55307\n",
      "Epoch 23/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.7525 - acc: 0.6518 - val_loss: 0.5337 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.55307 to 0.53374, saving model to gpr_classify.h5\n",
      "Epoch 24/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6862 - acc: 0.6894 - val_loss: 0.5512 - val_acc: 0.8013\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.53374\n",
      "Epoch 25/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6821 - acc: 0.6946 - val_loss: 0.5676 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.53374\n",
      "Epoch 26/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6970 - acc: 0.6886 - val_loss: 0.5275 - val_acc: 0.7821\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.53374 to 0.52746, saving model to gpr_classify.h5\n",
      "Epoch 27/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6487 - acc: 0.7179 - val_loss: 0.5207 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.52746 to 0.52074, saving model to gpr_classify.h5\n",
      "Epoch 28/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6404 - acc: 0.7218 - val_loss: 0.5215 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.52074\n",
      "Epoch 29/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6399 - acc: 0.7272 - val_loss: 0.5639 - val_acc: 0.7885\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.52074\n",
      "Epoch 30/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.6990 - acc: 0.6834 - val_loss: 0.5226 - val_acc: 0.7724\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.52074\n",
      "Epoch 31/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6977 - acc: 0.7000 - val_loss: 0.4871 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.52074 to 0.48706, saving model to gpr_classify.h5\n",
      "Epoch 32/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6668 - acc: 0.7147 - val_loss: 0.5114 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48706\n",
      "Epoch 33/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6305 - acc: 0.7347 - val_loss: 0.4871 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48706\n",
      "Epoch 34/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5913 - acc: 0.7492 - val_loss: 0.4394 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.48706 to 0.43937, saving model to gpr_classify.h5\n",
      "Epoch 35/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5840 - acc: 0.7596 - val_loss: 0.4969 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.43937\n",
      "Epoch 36/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5743 - acc: 0.7601 - val_loss: 0.5119 - val_acc: 0.8397\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.43937\n",
      "Epoch 37/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6114 - acc: 0.7402 - val_loss: 0.4663 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.43937\n",
      "Epoch 38/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6087 - acc: 0.7426 - val_loss: 0.5248 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.43937\n",
      "Epoch 39/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5817 - acc: 0.7448 - val_loss: 0.4324 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.43937 to 0.43240, saving model to gpr_classify.h5\n",
      "Epoch 40/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5674 - acc: 0.7586 - val_loss: 0.4648 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.43240\n",
      "Epoch 41/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5987 - acc: 0.7470 - val_loss: 0.4844 - val_acc: 0.8494\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.43240\n",
      "Epoch 42/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5803 - acc: 0.7595 - val_loss: 0.4108 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.43240 to 0.41077, saving model to gpr_classify.h5\n",
      "Epoch 43/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5450 - acc: 0.7714 - val_loss: 0.4280 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.41077\n",
      "Epoch 44/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6105 - acc: 0.7484 - val_loss: 0.6042 - val_acc: 0.7564\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.41077\n",
      "Epoch 45/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.6534 - acc: 0.7149 - val_loss: 0.4208 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.41077\n",
      "Epoch 46/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5768 - acc: 0.7552 - val_loss: 0.4383 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.41077\n",
      "Epoch 47/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5976 - acc: 0.7393 - val_loss: 0.4726 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.41077\n",
      "Epoch 48/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5896 - acc: 0.7418 - val_loss: 0.4276 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.41077\n",
      "Epoch 49/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5437 - acc: 0.7870 - val_loss: 0.4377 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.41077\n",
      "Epoch 50/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.5438 - acc: 0.7733 - val_loss: 0.3678 - val_acc: 0.8782\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.41077 to 0.36781, saving model to gpr_classify.h5\n",
      "Epoch 51/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5076 - acc: 0.7900 - val_loss: 0.3537 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.36781 to 0.35371, saving model to gpr_classify.h5\n",
      "Epoch 52/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5364 - acc: 0.7804 - val_loss: 0.4065 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.35371\n",
      "Epoch 53/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5015 - acc: 0.7941 - val_loss: 0.3635 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.35371\n",
      "Epoch 54/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5312 - acc: 0.7753 - val_loss: 0.3773 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.35371\n",
      "Epoch 55/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.5252 - acc: 0.7750 - val_loss: 0.4252 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.35371\n",
      "Epoch 56/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.5351 - acc: 0.7763 - val_loss: 0.3576 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.35371\n",
      "Epoch 57/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4908 - acc: 0.8024 - val_loss: 0.3500 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.35371 to 0.34998, saving model to gpr_classify.h5\n",
      "Epoch 58/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.5242 - acc: 0.7872 - val_loss: 0.4323 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.34998\n",
      "Epoch 59/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4917 - acc: 0.7917 - val_loss: 0.3909 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.34998\n",
      "Epoch 60/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.5090 - acc: 0.7931 - val_loss: 0.3345 - val_acc: 0.8782\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.34998 to 0.33449, saving model to gpr_classify.h5\n",
      "Epoch 61/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4976 - acc: 0.7931 - val_loss: 0.3735 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.33449\n",
      "Epoch 62/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4916 - acc: 0.7917 - val_loss: 0.3319 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.33449 to 0.33192, saving model to gpr_classify.h5\n",
      "Epoch 63/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.5100 - acc: 0.7867 - val_loss: 0.3723 - val_acc: 0.8301\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.33192\n",
      "Epoch 64/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.4733 - acc: 0.8262 - val_loss: 0.3246 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.33192 to 0.32464, saving model to gpr_classify.h5\n",
      "Epoch 65/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.4678 - acc: 0.8122 - val_loss: 0.3228 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.32464 to 0.32283, saving model to gpr_classify.h5\n",
      "Epoch 66/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4540 - acc: 0.8130 - val_loss: 0.3220 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.32283 to 0.32201, saving model to gpr_classify.h5\n",
      "Epoch 67/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4605 - acc: 0.8028 - val_loss: 0.2826 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.32201 to 0.28257, saving model to gpr_classify.h5\n",
      "Epoch 68/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4990 - acc: 0.7911 - val_loss: 0.3334 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.28257\n",
      "Epoch 69/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4768 - acc: 0.8131 - val_loss: 0.2902 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.28257\n",
      "Epoch 70/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4738 - acc: 0.7980 - val_loss: 0.2933 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.28257\n",
      "Epoch 71/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4591 - acc: 0.8059 - val_loss: 0.3193 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.28257\n",
      "Epoch 72/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4589 - acc: 0.8073 - val_loss: 0.2951 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.28257\n",
      "Epoch 73/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4064 - acc: 0.8298 - val_loss: 0.3878 - val_acc: 0.8429\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.28257\n",
      "Epoch 74/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4692 - acc: 0.8061 - val_loss: 0.2971 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.28257\n",
      "Epoch 75/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4636 - acc: 0.8265 - val_loss: 0.3203 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.28257\n",
      "Epoch 76/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4395 - acc: 0.8306 - val_loss: 0.3543 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.28257\n",
      "Epoch 77/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4248 - acc: 0.8199 - val_loss: 0.3020 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.28257\n",
      "Epoch 78/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4168 - acc: 0.8290 - val_loss: 0.2567 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.28257 to 0.25666, saving model to gpr_classify.h5\n",
      "Epoch 79/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.3929 - acc: 0.8350 - val_loss: 0.2627 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.25666\n",
      "Epoch 80/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4203 - acc: 0.8347 - val_loss: 0.2713 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.25666\n",
      "Epoch 81/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4112 - acc: 0.8331 - val_loss: 0.2454 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.25666 to 0.24535, saving model to gpr_classify.h5\n",
      "Epoch 82/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4240 - acc: 0.8282 - val_loss: 0.2592 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.24535\n",
      "Epoch 83/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4184 - acc: 0.8366 - val_loss: 0.2627 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.24535\n",
      "Epoch 84/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4105 - acc: 0.8454 - val_loss: 0.2830 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.24535\n",
      "Epoch 85/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4029 - acc: 0.8300 - val_loss: 0.2569 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.24535\n",
      "Epoch 86/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3989 - acc: 0.8229 - val_loss: 0.2630 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.24535\n",
      "Epoch 87/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3981 - acc: 0.8454 - val_loss: 0.2700 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.24535\n",
      "Epoch 88/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3749 - acc: 0.8486 - val_loss: 0.2436 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.24535 to 0.24364, saving model to gpr_classify.h5\n",
      "Epoch 89/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3835 - acc: 0.8492 - val_loss: 0.2863 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.24364\n",
      "Epoch 90/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4072 - acc: 0.8284 - val_loss: 0.2657 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.24364\n",
      "Epoch 91/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.3642 - acc: 0.8569 - val_loss: 0.2633 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.24364\n",
      "Epoch 92/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.4036 - acc: 0.8259 - val_loss: 0.2843 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.24364\n",
      "Epoch 93/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3801 - acc: 0.8542 - val_loss: 0.2107 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.24364 to 0.21068, saving model to gpr_classify.h5\n",
      "Epoch 94/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3665 - acc: 0.8566 - val_loss: 0.2283 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.21068\n",
      "Epoch 95/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.3817 - acc: 0.8528 - val_loss: 0.2091 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.21068 to 0.20911, saving model to gpr_classify.h5\n",
      "Epoch 96/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3469 - acc: 0.8731 - val_loss: 0.2061 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.20911 to 0.20606, saving model to gpr_classify.h5\n",
      "Epoch 97/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3415 - acc: 0.8547 - val_loss: 0.2145 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.20606\n",
      "Epoch 98/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3722 - acc: 0.8429 - val_loss: 0.2327 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.20606\n",
      "Epoch 99/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3417 - acc: 0.8687 - val_loss: 0.2863 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.20606\n",
      "Epoch 100/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3737 - acc: 0.8361 - val_loss: 0.2400 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.20606\n",
      "Epoch 101/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.3374 - acc: 0.8692 - val_loss: 0.2376 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.20606\n",
      "Epoch 102/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3640 - acc: 0.8611 - val_loss: 0.2097 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.20606\n",
      "Epoch 103/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3305 - acc: 0.8698 - val_loss: 0.2577 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.20606\n",
      "Epoch 104/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3385 - acc: 0.8634 - val_loss: 0.2784 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.20606\n",
      "Epoch 105/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3639 - acc: 0.8547 - val_loss: 0.2604 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.20606\n",
      "Epoch 106/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3422 - acc: 0.8687 - val_loss: 0.2431 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.20606\n",
      "Epoch 107/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3418 - acc: 0.8580 - val_loss: 0.2200 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.20606\n",
      "Epoch 108/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3509 - acc: 0.8717 - val_loss: 0.2553 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.20606\n",
      "Epoch 109/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3461 - acc: 0.8701 - val_loss: 0.2168 - val_acc: 0.9103\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.20606\n",
      "Epoch 110/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3520 - acc: 0.8692 - val_loss: 0.2578 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.20606\n",
      "Epoch 111/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3271 - acc: 0.8632 - val_loss: 0.1920 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.20606 to 0.19204, saving model to gpr_classify.h5\n",
      "Epoch 112/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3149 - acc: 0.8706 - val_loss: 0.2763 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.19204\n",
      "Epoch 113/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3291 - acc: 0.8689 - val_loss: 0.2145 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.19204\n",
      "Epoch 114/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3072 - acc: 0.8766 - val_loss: 0.1870 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.19204 to 0.18700, saving model to gpr_classify.h5\n",
      "Epoch 115/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3302 - acc: 0.8643 - val_loss: 0.2699 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.18700\n",
      "Epoch 116/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3438 - acc: 0.8731 - val_loss: 0.1797 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.18700 to 0.17965, saving model to gpr_classify.h5\n",
      "Epoch 117/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.2841 - acc: 0.8890 - val_loss: 0.1754 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.17965 to 0.17537, saving model to gpr_classify.h5\n",
      "Epoch 118/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3116 - acc: 0.8811 - val_loss: 0.2115 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.17537\n",
      "Epoch 119/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3312 - acc: 0.8725 - val_loss: 0.1975 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.17537\n",
      "Epoch 120/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3122 - acc: 0.8728 - val_loss: 0.1891 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.17537\n",
      "Epoch 121/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3044 - acc: 0.8816 - val_loss: 0.2590 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.17537\n",
      "Epoch 122/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3034 - acc: 0.8783 - val_loss: 0.1937 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.17537\n",
      "Epoch 123/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3176 - acc: 0.8720 - val_loss: 0.3643 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.17537\n",
      "Epoch 124/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3262 - acc: 0.8733 - val_loss: 0.1897 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.17537\n",
      "Epoch 125/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3186 - acc: 0.8775 - val_loss: 0.2918 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.17537\n",
      "Epoch 126/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3109 - acc: 0.8799 - val_loss: 0.2240 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.17537\n",
      "Epoch 127/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2702 - acc: 0.8813 - val_loss: 0.2233 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.17537\n",
      "Epoch 128/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3353 - acc: 0.8750 - val_loss: 0.1978 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.17537\n",
      "Epoch 129/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3164 - acc: 0.8701 - val_loss: 0.2675 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.17537\n",
      "Epoch 130/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3106 - acc: 0.8805 - val_loss: 0.2034 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17537\n",
      "Epoch 131/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.3637 - acc: 0.8520 - val_loss: 0.2203 - val_acc: 0.8782\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17537\n",
      "Epoch 132/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.3387 - acc: 0.8848 - val_loss: 0.2082 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17537\n",
      "Epoch 133/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.3102 - acc: 0.8844 - val_loss: 0.1752 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.17537 to 0.17520, saving model to gpr_classify.h5\n",
      "Epoch 134/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2903 - acc: 0.8870 - val_loss: 0.1665 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.17520 to 0.16651, saving model to gpr_classify.h5\n",
      "Epoch 135/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2752 - acc: 0.8901 - val_loss: 0.1787 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.16651\n",
      "Epoch 136/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2836 - acc: 0.8936 - val_loss: 0.1926 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.16651\n",
      "Epoch 137/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2735 - acc: 0.8870 - val_loss: 0.1993 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.16651\n",
      "Epoch 138/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2870 - acc: 0.8936 - val_loss: 0.1814 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.16651\n",
      "Epoch 139/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2781 - acc: 0.8939 - val_loss: 0.1657 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.16651 to 0.16574, saving model to gpr_classify.h5\n",
      "Epoch 140/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2554 - acc: 0.9032 - val_loss: 0.1462 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.16574 to 0.14617, saving model to gpr_classify.h5\n",
      "Epoch 141/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2878 - acc: 0.8964 - val_loss: 0.1568 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.14617\n",
      "Epoch 142/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2763 - acc: 0.8866 - val_loss: 0.1835 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.14617\n",
      "Epoch 143/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.3064 - acc: 0.8756 - val_loss: 0.1587 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.14617\n",
      "Epoch 144/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2727 - acc: 0.8917 - val_loss: 0.2106 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.14617\n",
      "Epoch 145/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2993 - acc: 0.8766 - val_loss: 0.1624 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.14617\n",
      "Epoch 146/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.2604 - acc: 0.9041 - val_loss: 0.1507 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.14617\n",
      "Epoch 147/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2569 - acc: 0.8926 - val_loss: 0.1623 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.14617\n",
      "Epoch 148/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2395 - acc: 0.9117 - val_loss: 0.1606 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.14617\n",
      "Epoch 149/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2278 - acc: 0.9202 - val_loss: 0.1819 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.14617\n",
      "Epoch 150/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2555 - acc: 0.9066 - val_loss: 0.1663 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.14617\n",
      "Epoch 151/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2494 - acc: 0.9139 - val_loss: 0.1597 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.14617\n",
      "Epoch 152/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2141 - acc: 0.9205 - val_loss: 0.1895 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.14617\n",
      "Epoch 153/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2440 - acc: 0.9013 - val_loss: 0.1459 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.14617 to 0.14591, saving model to gpr_classify.h5\n",
      "Epoch 154/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.2328 - acc: 0.9181 - val_loss: 0.1813 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.14591\n",
      "Epoch 155/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2437 - acc: 0.8991 - val_loss: 0.1317 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.14591 to 0.13174, saving model to gpr_classify.h5\n",
      "Epoch 156/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2390 - acc: 0.9082 - val_loss: 0.1754 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.13174\n",
      "Epoch 157/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2656 - acc: 0.8915 - val_loss: 0.1907 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.13174\n",
      "Epoch 158/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2524 - acc: 0.9035 - val_loss: 0.1569 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.13174\n",
      "Epoch 159/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2450 - acc: 0.8994 - val_loss: 0.1412 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.13174\n",
      "Epoch 160/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2243 - acc: 0.9131 - val_loss: 0.1319 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.13174\n",
      "Epoch 161/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2390 - acc: 0.9084 - val_loss: 0.1465 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.13174\n",
      "Epoch 162/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2304 - acc: 0.9051 - val_loss: 0.2608 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.13174\n",
      "Epoch 163/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2327 - acc: 0.9161 - val_loss: 0.1341 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.13174\n",
      "Epoch 164/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2380 - acc: 0.9084 - val_loss: 0.2031 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.13174\n",
      "Epoch 165/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2450 - acc: 0.9027 - val_loss: 0.1313 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.13174 to 0.13129, saving model to gpr_classify.h5\n",
      "Epoch 166/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2325 - acc: 0.9099 - val_loss: 0.1311 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.13129 to 0.13110, saving model to gpr_classify.h5\n",
      "Epoch 167/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.2278 - acc: 0.9150 - val_loss: 0.1218 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.13110 to 0.12183, saving model to gpr_classify.h5\n",
      "Epoch 168/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1995 - acc: 0.9284 - val_loss: 0.1687 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.12183\n",
      "Epoch 169/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2608 - acc: 0.8884 - val_loss: 0.1395 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.12183\n",
      "Epoch 170/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2582 - acc: 0.8936 - val_loss: 0.2152 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.12183\n",
      "Epoch 171/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2095 - acc: 0.9194 - val_loss: 0.1390 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.12183\n",
      "Epoch 172/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2065 - acc: 0.9268 - val_loss: 0.1251 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.12183\n",
      "Epoch 173/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.1985 - acc: 0.9247 - val_loss: 0.1097 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.12183 to 0.10971, saving model to gpr_classify.h5\n",
      "Epoch 174/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2069 - acc: 0.9131 - val_loss: 0.1208 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.10971\n",
      "Epoch 175/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2293 - acc: 0.9112 - val_loss: 0.1243 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.10971\n",
      "Epoch 176/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2070 - acc: 0.9156 - val_loss: 0.1205 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.10971\n",
      "Epoch 177/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1991 - acc: 0.9271 - val_loss: 0.1190 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.10971\n",
      "Epoch 178/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2265 - acc: 0.9137 - val_loss: 0.2878 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.10971\n",
      "Epoch 179/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1909 - acc: 0.9323 - val_loss: 0.1073 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.10971 to 0.10729, saving model to gpr_classify.h5\n",
      "Epoch 180/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.1651 - acc: 0.9386 - val_loss: 0.0799 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.10729 to 0.07989, saving model to gpr_classify.h5\n",
      "Epoch 181/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.1920 - acc: 0.9180 - val_loss: 0.0898 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.07989\n",
      "Epoch 182/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1944 - acc: 0.9224 - val_loss: 0.1317 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.07989\n",
      "Epoch 183/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2145 - acc: 0.9180 - val_loss: 0.1317 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.07989\n",
      "Epoch 184/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1706 - acc: 0.9342 - val_loss: 0.0898 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.07989\n",
      "Epoch 185/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1897 - acc: 0.9273 - val_loss: 0.1095 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.07989\n",
      "Epoch 186/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1841 - acc: 0.9317 - val_loss: 0.1484 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.07989\n",
      "Epoch 187/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1978 - acc: 0.9208 - val_loss: 0.1042 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.07989\n",
      "Epoch 188/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1969 - acc: 0.9279 - val_loss: 0.1374 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.07989\n",
      "Epoch 189/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1943 - acc: 0.9225 - val_loss: 0.1045 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.07989\n",
      "Epoch 190/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1938 - acc: 0.9295 - val_loss: 0.1210 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.07989\n",
      "Epoch 191/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1639 - acc: 0.9336 - val_loss: 0.0866 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.07989\n",
      "Epoch 192/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1657 - acc: 0.9336 - val_loss: 0.0821 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.07989\n",
      "Epoch 193/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2033 - acc: 0.9221 - val_loss: 0.1634 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.07989\n",
      "Epoch 194/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1992 - acc: 0.9262 - val_loss: 0.0903 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.07989\n",
      "Epoch 195/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.2143 - acc: 0.9233 - val_loss: 0.1040 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.07989\n",
      "Epoch 196/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1727 - acc: 0.9320 - val_loss: 0.1180 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.07989\n",
      "Epoch 197/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1967 - acc: 0.9213 - val_loss: 0.1072 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.07989\n",
      "Epoch 198/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1824 - acc: 0.9345 - val_loss: 0.0902 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.07989\n",
      "Epoch 199/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2112 - acc: 0.9293 - val_loss: 0.1059 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.07989\n",
      "Epoch 200/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1955 - acc: 0.9281 - val_loss: 0.1262 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.07989\n",
      "Epoch 201/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2091 - acc: 0.9243 - val_loss: 0.1287 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.07989\n",
      "Epoch 202/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1847 - acc: 0.9290 - val_loss: 0.1122 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.07989\n",
      "Epoch 203/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1872 - acc: 0.9290 - val_loss: 0.0916 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.07989\n",
      "Epoch 204/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1709 - acc: 0.9400 - val_loss: 0.1163 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.07989\n",
      "Epoch 205/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1429 - acc: 0.9452 - val_loss: 0.0895 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.07989\n",
      "Epoch 206/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1402 - acc: 0.9495 - val_loss: 0.1022 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.07989\n",
      "Epoch 207/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1783 - acc: 0.9419 - val_loss: 0.0768 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.07989 to 0.07680, saving model to gpr_classify.h5\n",
      "Epoch 208/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1646 - acc: 0.9408 - val_loss: 0.0800 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.07680\n",
      "Epoch 209/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1700 - acc: 0.9364 - val_loss: 0.0732 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.07680 to 0.07319, saving model to gpr_classify.h5\n",
      "Epoch 210/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1411 - acc: 0.9476 - val_loss: 0.0831 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.07319\n",
      "Epoch 211/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1572 - acc: 0.9427 - val_loss: 0.1110 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.07319\n",
      "Epoch 212/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1589 - acc: 0.9389 - val_loss: 0.0672 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.07319 to 0.06715, saving model to gpr_classify.h5\n",
      "Epoch 213/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1566 - acc: 0.9430 - val_loss: 0.0889 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.06715\n",
      "Epoch 214/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1668 - acc: 0.9422 - val_loss: 0.0564 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.06715 to 0.05639, saving model to gpr_classify.h5\n",
      "Epoch 215/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.2238 - acc: 0.9122 - val_loss: 0.1437 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.05639\n",
      "Epoch 216/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1756 - acc: 0.9304 - val_loss: 0.0827 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.05639\n",
      "Epoch 217/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1691 - acc: 0.9367 - val_loss: 0.1025 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.05639\n",
      "Epoch 218/500\n",
      "19/19 [==============================] - 63s 3s/step - loss: 0.1693 - acc: 0.9364 - val_loss: 0.0908 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.05639\n",
      "Epoch 219/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1986 - acc: 0.9252 - val_loss: 0.0963 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.05639\n",
      "Epoch 220/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1696 - acc: 0.9372 - val_loss: 0.1170 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.05639\n",
      "Epoch 221/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1528 - acc: 0.9356 - val_loss: 0.0836 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.05639\n",
      "Epoch 222/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1657 - acc: 0.9328 - val_loss: 0.1049 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.05639\n",
      "Epoch 223/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1291 - acc: 0.9531 - val_loss: 0.0578 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.05639\n",
      "Epoch 224/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1610 - acc: 0.9345 - val_loss: 0.0687 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.05639\n",
      "Epoch 225/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1450 - acc: 0.9405 - val_loss: 0.0778 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.05639\n",
      "Epoch 226/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1646 - acc: 0.9460 - val_loss: 0.0702 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.05639\n",
      "Epoch 227/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1382 - acc: 0.9540 - val_loss: 0.0598 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.05639\n",
      "Epoch 228/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1250 - acc: 0.9545 - val_loss: 0.0894 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.05639\n",
      "Epoch 229/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1534 - acc: 0.9490 - val_loss: 0.0884 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.05639\n",
      "Epoch 230/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1262 - acc: 0.9479 - val_loss: 0.0788 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.05639\n",
      "Epoch 231/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1567 - acc: 0.9463 - val_loss: 0.0670 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.05639\n",
      "Epoch 232/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1630 - acc: 0.9331 - val_loss: 0.0690 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.05639\n",
      "Epoch 233/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1330 - acc: 0.9455 - val_loss: 0.0919 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.05639\n",
      "Epoch 234/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1431 - acc: 0.9455 - val_loss: 0.0847 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.05639\n",
      "Epoch 235/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1301 - acc: 0.9531 - val_loss: 0.1259 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.05639\n",
      "Epoch 236/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1379 - acc: 0.9474 - val_loss: 0.0743 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.05639\n",
      "Epoch 237/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1293 - acc: 0.9487 - val_loss: 0.0766 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.05639\n",
      "Epoch 238/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1497 - acc: 0.9495 - val_loss: 0.1216 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.05639\n",
      "Epoch 239/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1363 - acc: 0.9545 - val_loss: 0.0854 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.05639\n",
      "Epoch 240/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1450 - acc: 0.9518 - val_loss: 0.0956 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.05639\n",
      "Epoch 241/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1119 - acc: 0.9619 - val_loss: 0.0795 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.05639\n",
      "Epoch 242/500\n",
      "19/19 [==============================] - 63s 3s/step - loss: 0.1398 - acc: 0.9528 - val_loss: 0.0612 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.05639\n",
      "Epoch 243/500\n",
      "19/19 [==============================] - 63s 3s/step - loss: 0.1450 - acc: 0.9526 - val_loss: 0.0934 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.05639\n",
      "Epoch 244/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1415 - acc: 0.9468 - val_loss: 0.0563 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.05639 to 0.05630, saving model to gpr_classify.h5\n",
      "Epoch 245/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1332 - acc: 0.9627 - val_loss: 0.0790 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.05630\n",
      "Epoch 246/500\n",
      "19/19 [==============================] - 63s 3s/step - loss: 0.1309 - acc: 0.9517 - val_loss: 0.0442 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.05630 to 0.04419, saving model to gpr_classify.h5\n",
      "Epoch 247/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1380 - acc: 0.9490 - val_loss: 0.0631 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.04419\n",
      "Epoch 248/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1366 - acc: 0.9493 - val_loss: 0.0927 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.04419\n",
      "Epoch 249/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1421 - acc: 0.9521 - val_loss: 0.0591 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.04419\n",
      "Epoch 250/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1217 - acc: 0.9578 - val_loss: 0.0644 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.04419\n",
      "Epoch 251/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1680 - acc: 0.9361 - val_loss: 0.0488 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.04419\n",
      "Epoch 252/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1570 - acc: 0.9487 - val_loss: 0.0865 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.04419\n",
      "Epoch 253/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1238 - acc: 0.9537 - val_loss: 0.0511 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.04419\n",
      "Epoch 254/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1230 - acc: 0.9578 - val_loss: 0.0561 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.04419\n",
      "Epoch 255/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0959 - acc: 0.9731 - val_loss: 0.0441 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.04419 to 0.04405, saving model to gpr_classify.h5\n",
      "Epoch 256/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1169 - acc: 0.9600 - val_loss: 0.0480 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.04405\n",
      "Epoch 257/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1037 - acc: 0.9603 - val_loss: 0.0586 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.04405\n",
      "Epoch 258/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1316 - acc: 0.9479 - val_loss: 0.0440 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.04405 to 0.04403, saving model to gpr_classify.h5\n",
      "Epoch 259/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1577 - acc: 0.9414 - val_loss: 0.0542 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.04403\n",
      "Epoch 260/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1245 - acc: 0.9594 - val_loss: 0.0610 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.04403\n",
      "Epoch 261/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1391 - acc: 0.9543 - val_loss: 0.0970 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.04403\n",
      "Epoch 262/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1200 - acc: 0.9561 - val_loss: 0.0418 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.04403 to 0.04179, saving model to gpr_classify.h5\n",
      "Epoch 263/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1184 - acc: 0.9493 - val_loss: 0.0526 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.04179\n",
      "Epoch 264/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1307 - acc: 0.9447 - val_loss: 0.0384 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.04179 to 0.03841, saving model to gpr_classify.h5\n",
      "Epoch 265/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0963 - acc: 0.9704 - val_loss: 0.0430 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.03841\n",
      "Epoch 266/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0804 - acc: 0.9759 - val_loss: 0.0364 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.03841 to 0.03638, saving model to gpr_classify.h5\n",
      "Epoch 267/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1187 - acc: 0.9616 - val_loss: 0.0649 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.03638\n",
      "Epoch 268/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1094 - acc: 0.9693 - val_loss: 0.0536 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.03638\n",
      "Epoch 269/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1131 - acc: 0.9608 - val_loss: 0.0332 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.03638 to 0.03320, saving model to gpr_classify.h5\n",
      "Epoch 270/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1104 - acc: 0.9652 - val_loss: 0.0415 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.03320\n",
      "Epoch 271/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1030 - acc: 0.9605 - val_loss: 0.0489 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.03320\n",
      "Epoch 272/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1271 - acc: 0.9515 - val_loss: 0.0326 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.03320 to 0.03262, saving model to gpr_classify.h5\n",
      "Epoch 273/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1482 - acc: 0.9443 - val_loss: 0.0738 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.03262\n",
      "Epoch 274/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1348 - acc: 0.9498 - val_loss: 0.0624 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.03262\n",
      "Epoch 275/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1267 - acc: 0.9572 - val_loss: 0.0412 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.03262\n",
      "Epoch 276/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1058 - acc: 0.9666 - val_loss: 0.0365 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.03262\n",
      "Epoch 277/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1192 - acc: 0.9605 - val_loss: 0.1084 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.03262\n",
      "Epoch 278/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1222 - acc: 0.9581 - val_loss: 0.0385 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.03262\n",
      "Epoch 279/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1099 - acc: 0.9676 - val_loss: 0.0454 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.03262\n",
      "Epoch 280/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1205 - acc: 0.9543 - val_loss: 0.0534 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.03262\n",
      "Epoch 281/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1022 - acc: 0.9627 - val_loss: 0.0747 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.03262\n",
      "Epoch 282/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1272 - acc: 0.9592 - val_loss: 0.0464 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.03262\n",
      "Epoch 283/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1180 - acc: 0.9635 - val_loss: 0.0659 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.03262\n",
      "Epoch 284/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1567 - acc: 0.9504 - val_loss: 0.0431 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.03262\n",
      "Epoch 285/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0881 - acc: 0.9720 - val_loss: 0.0289 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.03262 to 0.02891, saving model to gpr_classify.h5\n",
      "Epoch 286/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0973 - acc: 0.9718 - val_loss: 0.0384 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.02891\n",
      "Epoch 287/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1053 - acc: 0.9646 - val_loss: 0.0425 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.02891\n",
      "Epoch 288/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0980 - acc: 0.9671 - val_loss: 0.0356 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.02891\n",
      "Epoch 289/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1229 - acc: 0.9539 - val_loss: 0.0332 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.02891\n",
      "Epoch 290/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0974 - acc: 0.9636 - val_loss: 0.0532 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.02891\n",
      "Epoch 291/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0923 - acc: 0.9649 - val_loss: 0.0503 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.02891\n",
      "Epoch 292/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1140 - acc: 0.9564 - val_loss: 0.0362 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.02891\n",
      "Epoch 293/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0846 - acc: 0.9723 - val_loss: 0.0308 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.02891\n",
      "Epoch 294/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0816 - acc: 0.9696 - val_loss: 0.0292 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.02891\n",
      "Epoch 295/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0984 - acc: 0.9682 - val_loss: 0.0424 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.02891\n",
      "Epoch 296/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1037 - acc: 0.9616 - val_loss: 0.0368 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.02891\n",
      "Epoch 297/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1010 - acc: 0.9619 - val_loss: 0.0656 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.02891\n",
      "Epoch 298/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.1131 - acc: 0.9567 - val_loss: 0.0355 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.02891\n",
      "Epoch 299/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1006 - acc: 0.9666 - val_loss: 0.0296 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.02891\n",
      "Epoch 300/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0892 - acc: 0.9712 - val_loss: 0.0208 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.02891 to 0.02077, saving model to gpr_classify.h5\n",
      "Epoch 301/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1342 - acc: 0.9551 - val_loss: 0.0441 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.02077\n",
      "Epoch 302/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.2021 - acc: 0.9339 - val_loss: 0.0674 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.02077\n",
      "Epoch 303/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1565 - acc: 0.9441 - val_loss: 0.0797 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.02077\n",
      "Epoch 304/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1450 - acc: 0.9471 - val_loss: 0.0627 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.02077\n",
      "Epoch 305/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1121 - acc: 0.9622 - val_loss: 0.0308 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.02077\n",
      "Epoch 306/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0905 - acc: 0.9644 - val_loss: 0.0281 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.02077\n",
      "Epoch 307/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0993 - acc: 0.9704 - val_loss: 0.0361 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.02077\n",
      "Epoch 308/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0870 - acc: 0.9751 - val_loss: 0.0323 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.02077\n",
      "Epoch 309/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0870 - acc: 0.9704 - val_loss: 0.0241 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.02077\n",
      "Epoch 310/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0840 - acc: 0.9753 - val_loss: 0.0257 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.02077\n",
      "Epoch 311/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0900 - acc: 0.9690 - val_loss: 0.0364 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.02077\n",
      "Epoch 312/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1049 - acc: 0.9633 - val_loss: 0.0433 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.02077\n",
      "Epoch 313/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1231 - acc: 0.9655 - val_loss: 0.0668 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.02077\n",
      "Epoch 314/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1027 - acc: 0.9633 - val_loss: 0.0257 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.02077\n",
      "Epoch 315/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0847 - acc: 0.9685 - val_loss: 0.0539 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.02077\n",
      "Epoch 316/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0895 - acc: 0.9660 - val_loss: 0.0264 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.02077\n",
      "Epoch 317/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0722 - acc: 0.9715 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.02077 to 0.01533, saving model to gpr_classify.h5\n",
      "Epoch 318/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0832 - acc: 0.9685 - val_loss: 0.0234 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.01533\n",
      "Epoch 319/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0820 - acc: 0.9731 - val_loss: 0.0310 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.01533\n",
      "Epoch 320/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0756 - acc: 0.9759 - val_loss: 0.0327 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.01533\n",
      "Epoch 321/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0812 - acc: 0.9753 - val_loss: 0.0246 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.01533\n",
      "Epoch 322/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0866 - acc: 0.9718 - val_loss: 0.0392 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.01533\n",
      "Epoch 323/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0851 - acc: 0.9764 - val_loss: 0.0203 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.01533\n",
      "Epoch 324/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1145 - acc: 0.9537 - val_loss: 0.0287 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.01533\n",
      "Epoch 325/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0941 - acc: 0.9729 - val_loss: 0.0434 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.01533\n",
      "Epoch 326/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.1067 - acc: 0.9644 - val_loss: 0.0811 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.01533\n",
      "Epoch 327/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0943 - acc: 0.9624 - val_loss: 0.0449 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.01533\n",
      "Epoch 328/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0798 - acc: 0.9707 - val_loss: 0.0257 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.01533\n",
      "Epoch 329/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0806 - acc: 0.9676 - val_loss: 0.0437 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.01533\n",
      "Epoch 330/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0928 - acc: 0.9687 - val_loss: 0.0297 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.01533\n",
      "Epoch 331/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0867 - acc: 0.9693 - val_loss: 0.0259 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.01533\n",
      "Epoch 332/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0939 - acc: 0.9638 - val_loss: 0.0390 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.01533\n",
      "Epoch 333/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0837 - acc: 0.9685 - val_loss: 0.0246 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.01533\n",
      "Epoch 334/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0851 - acc: 0.9726 - val_loss: 0.0243 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.01533\n",
      "Epoch 335/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0822 - acc: 0.9704 - val_loss: 0.0514 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.01533\n",
      "Epoch 336/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1154 - acc: 0.9589 - val_loss: 0.0309 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.01533\n",
      "Epoch 337/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0836 - acc: 0.9728 - val_loss: 0.0286 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.01533\n",
      "Epoch 338/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0688 - acc: 0.9797 - val_loss: 0.0282 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.01533\n",
      "Epoch 339/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0684 - acc: 0.9770 - val_loss: 0.0322 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.01533\n",
      "Epoch 340/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0786 - acc: 0.9726 - val_loss: 0.0173 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.01533\n",
      "Epoch 341/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0813 - acc: 0.9786 - val_loss: 0.0201 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.01533\n",
      "Epoch 342/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0790 - acc: 0.9707 - val_loss: 0.0466 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.01533\n",
      "Epoch 343/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0806 - acc: 0.9745 - val_loss: 0.0276 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.01533\n",
      "Epoch 344/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0796 - acc: 0.9707 - val_loss: 0.0195 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.01533\n",
      "Epoch 345/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0783 - acc: 0.9740 - val_loss: 0.0213 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.01533\n",
      "Epoch 346/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0876 - acc: 0.9729 - val_loss: 0.0297 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.01533\n",
      "Epoch 347/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0766 - acc: 0.9707 - val_loss: 0.0324 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.01533\n",
      "Epoch 348/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0701 - acc: 0.9825 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.01533 to 0.01496, saving model to gpr_classify.h5\n",
      "Epoch 349/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0714 - acc: 0.9767 - val_loss: 0.0216 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.01496\n",
      "Epoch 350/500\n",
      "19/19 [==============================] - 67s 4s/step - loss: 0.0729 - acc: 0.9734 - val_loss: 0.0244 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.01496\n",
      "Epoch 351/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0701 - acc: 0.9764 - val_loss: 0.0190 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.01496\n",
      "Epoch 352/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0753 - acc: 0.9726 - val_loss: 0.0618 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.01496\n",
      "Epoch 353/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0854 - acc: 0.9734 - val_loss: 0.0439 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.01496\n",
      "Epoch 354/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0620 - acc: 0.9794 - val_loss: 0.0197 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.01496\n",
      "Epoch 355/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0884 - acc: 0.9693 - val_loss: 0.0546 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.01496\n",
      "Epoch 356/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0883 - acc: 0.9699 - val_loss: 0.0391 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.01496\n",
      "Epoch 357/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0687 - acc: 0.9778 - val_loss: 0.0167 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.01496\n",
      "Epoch 358/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0911 - acc: 0.9721 - val_loss: 0.0372 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.01496\n",
      "Epoch 359/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0675 - acc: 0.9786 - val_loss: 0.0207 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.01496\n",
      "Epoch 360/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0677 - acc: 0.9745 - val_loss: 0.0234 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.01496\n",
      "Epoch 361/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0627 - acc: 0.9792 - val_loss: 0.0218 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.01496\n",
      "Epoch 362/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0899 - acc: 0.9740 - val_loss: 0.0208 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.01496\n",
      "Epoch 363/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1272 - acc: 0.9616 - val_loss: 0.0264 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.01496\n",
      "Epoch 364/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0867 - acc: 0.9666 - val_loss: 0.0286 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.01496\n",
      "Epoch 365/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0654 - acc: 0.9745 - val_loss: 0.0491 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.01496\n",
      "Epoch 366/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.1288 - acc: 0.9548 - val_loss: 0.1049 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.01496\n",
      "Epoch 367/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1063 - acc: 0.9635 - val_loss: 0.0399 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.01496\n",
      "Epoch 368/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0875 - acc: 0.9688 - val_loss: 0.0304 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.01496\n",
      "Epoch 369/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0799 - acc: 0.9685 - val_loss: 0.0243 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.01496\n",
      "Epoch 370/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0873 - acc: 0.9726 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.01496 to 0.01345, saving model to gpr_classify.h5\n",
      "Epoch 371/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0830 - acc: 0.9699 - val_loss: 0.0217 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.01345\n",
      "Epoch 372/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0678 - acc: 0.9751 - val_loss: 0.0219 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.01345\n",
      "Epoch 373/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0639 - acc: 0.9786 - val_loss: 0.0189 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.01345\n",
      "Epoch 374/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0727 - acc: 0.9754 - val_loss: 0.0290 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.01345\n",
      "Epoch 375/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1146 - acc: 0.9624 - val_loss: 0.0858 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.01345\n",
      "Epoch 376/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0688 - acc: 0.9792 - val_loss: 0.0266 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.01345\n",
      "Epoch 377/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0803 - acc: 0.9707 - val_loss: 0.0264 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.01345\n",
      "Epoch 378/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0816 - acc: 0.9742 - val_loss: 0.0396 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.01345\n",
      "Epoch 379/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0658 - acc: 0.9800 - val_loss: 0.0194 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.01345\n",
      "Epoch 380/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0542 - acc: 0.9803 - val_loss: 0.0212 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.01345\n",
      "Epoch 381/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0663 - acc: 0.9759 - val_loss: 0.0217 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.01345\n",
      "Epoch 382/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0628 - acc: 0.9786 - val_loss: 0.0198 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.01345\n",
      "Epoch 383/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0626 - acc: 0.9819 - val_loss: 0.0177 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.01345\n",
      "Epoch 384/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0670 - acc: 0.9764 - val_loss: 0.0147 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.01345\n",
      "Epoch 385/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0832 - acc: 0.9704 - val_loss: 0.0576 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.01345\n",
      "Epoch 386/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0683 - acc: 0.9742 - val_loss: 0.0344 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.01345\n",
      "Epoch 387/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0640 - acc: 0.9803 - val_loss: 0.0237 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.01345\n",
      "Epoch 388/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0613 - acc: 0.9783 - val_loss: 0.0189 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.01345\n",
      "Epoch 389/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0922 - acc: 0.9663 - val_loss: 0.0341 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.01345\n",
      "Epoch 390/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0723 - acc: 0.9767 - val_loss: 0.0135 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.01345\n",
      "Epoch 391/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0638 - acc: 0.9783 - val_loss: 0.0230 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.01345\n",
      "Epoch 392/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0551 - acc: 0.9835 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.01345 to 0.01340, saving model to gpr_classify.h5\n",
      "Epoch 393/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0596 - acc: 0.9794 - val_loss: 0.0280 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.01340\n",
      "Epoch 394/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0904 - acc: 0.9696 - val_loss: 0.0683 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.01340\n",
      "Epoch 395/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0782 - acc: 0.9751 - val_loss: 0.0284 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.01340\n",
      "Epoch 396/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0759 - acc: 0.9775 - val_loss: 0.0306 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.01340\n",
      "Epoch 397/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0717 - acc: 0.9721 - val_loss: 0.0267 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.01340\n",
      "Epoch 398/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0687 - acc: 0.9751 - val_loss: 0.0197 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.01340\n",
      "Epoch 399/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0791 - acc: 0.9671 - val_loss: 0.0274 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.01340\n",
      "Epoch 400/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0732 - acc: 0.9742 - val_loss: 0.0195 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.01340\n",
      "Epoch 401/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0726 - acc: 0.9756 - val_loss: 0.0449 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.01340\n",
      "Epoch 402/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0813 - acc: 0.9687 - val_loss: 0.0129 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00402: val_loss improved from 0.01340 to 0.01286, saving model to gpr_classify.h5\n",
      "Epoch 403/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0503 - acc: 0.9858 - val_loss: 0.0315 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.01286\n",
      "Epoch 404/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0582 - acc: 0.9827 - val_loss: 0.0175 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.01286\n",
      "Epoch 405/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0666 - acc: 0.9742 - val_loss: 0.0290 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.01286\n",
      "Epoch 406/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0738 - acc: 0.9734 - val_loss: 0.0397 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.01286\n",
      "Epoch 407/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0503 - acc: 0.9827 - val_loss: 0.0222 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.01286\n",
      "Epoch 408/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0751 - acc: 0.9728 - val_loss: 0.0216 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.01286\n",
      "Epoch 409/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0734 - acc: 0.9701 - val_loss: 0.0135 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.01286\n",
      "Epoch 410/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0717 - acc: 0.9803 - val_loss: 0.0180 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.01286\n",
      "Epoch 411/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0837 - acc: 0.9737 - val_loss: 0.0149 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.01286\n",
      "Epoch 412/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0997 - acc: 0.9652 - val_loss: 0.0280 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.01286\n",
      "Epoch 413/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0755 - acc: 0.9781 - val_loss: 0.0386 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.01286\n",
      "Epoch 414/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0599 - acc: 0.9751 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.01286\n",
      "Epoch 415/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0730 - acc: 0.9745 - val_loss: 0.0199 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.01286\n",
      "Epoch 416/500\n",
      "19/19 [==============================] - 68s 4s/step - loss: 0.0617 - acc: 0.9786 - val_loss: 0.0256 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.01286\n",
      "Epoch 417/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0746 - acc: 0.9764 - val_loss: 0.0124 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.01286 to 0.01236, saving model to gpr_classify.h5\n",
      "Epoch 418/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0674 - acc: 0.9745 - val_loss: 0.0258 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.01236\n",
      "Epoch 419/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0522 - acc: 0.9835 - val_loss: 0.0141 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.01236\n",
      "Epoch 420/500\n",
      "19/19 [==============================] - 64s 3s/step - loss: 0.0619 - acc: 0.9775 - val_loss: 0.0112 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00420: val_loss improved from 0.01236 to 0.01124, saving model to gpr_classify.h5\n",
      "Epoch 421/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0616 - acc: 0.9797 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00421: val_loss improved from 0.01124 to 0.00885, saving model to gpr_classify.h5\n",
      "Epoch 422/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0685 - acc: 0.9759 - val_loss: 0.0265 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.00885\n",
      "Epoch 423/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0671 - acc: 0.9759 - val_loss: 0.0240 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.00885\n",
      "Epoch 424/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0627 - acc: 0.9786 - val_loss: 0.0202 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.00885\n",
      "Epoch 425/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0635 - acc: 0.9783 - val_loss: 0.0227 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.00885\n",
      "Epoch 426/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0537 - acc: 0.9811 - val_loss: 0.0143 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.00885\n",
      "Epoch 427/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0547 - acc: 0.9792 - val_loss: 0.0157 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.00885\n",
      "Epoch 428/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0576 - acc: 0.9778 - val_loss: 0.0225 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.00885\n",
      "Epoch 429/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0559 - acc: 0.9794 - val_loss: 0.0182 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.00885\n",
      "Epoch 430/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0538 - acc: 0.9833 - val_loss: 0.0219 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.00885\n",
      "Epoch 431/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0562 - acc: 0.9849 - val_loss: 0.0174 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.00885\n",
      "Epoch 432/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0902 - acc: 0.9701 - val_loss: 0.0360 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.00885\n",
      "Epoch 433/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0702 - acc: 0.9740 - val_loss: 0.0954 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.00885\n",
      "Epoch 434/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0451 - acc: 0.9835 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.00885\n",
      "Epoch 435/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0428 - acc: 0.9835 - val_loss: 0.0146 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.00885\n",
      "Epoch 436/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0600 - acc: 0.9835 - val_loss: 0.0340 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.00885\n",
      "Epoch 437/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0595 - acc: 0.9803 - val_loss: 0.0285 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.00885\n",
      "Epoch 438/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0379 - acc: 0.9885 - val_loss: 0.0237 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.00885\n",
      "Epoch 439/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0631 - acc: 0.9775 - val_loss: 0.0394 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.00885\n",
      "Epoch 440/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0611 - acc: 0.9822 - val_loss: 0.0478 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.00885\n",
      "Epoch 441/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0560 - acc: 0.9803 - val_loss: 0.0364 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.00885\n",
      "Epoch 442/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0574 - acc: 0.9803 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.00885\n",
      "Epoch 443/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0577 - acc: 0.9822 - val_loss: 0.0562 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.00885\n",
      "Epoch 444/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0888 - acc: 0.9770 - val_loss: 0.0429 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.00885\n",
      "Epoch 445/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.1063 - acc: 0.9649 - val_loss: 0.0797 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.00885\n",
      "Epoch 446/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0890 - acc: 0.9701 - val_loss: 0.0368 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.00885\n",
      "Epoch 447/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0819 - acc: 0.9701 - val_loss: 0.0264 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.00885\n",
      "Epoch 448/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0526 - acc: 0.9819 - val_loss: 0.0171 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.00885\n",
      "Epoch 449/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0493 - acc: 0.9882 - val_loss: 0.0287 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.00885\n",
      "Epoch 450/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0502 - acc: 0.9816 - val_loss: 0.0185 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.00885\n",
      "Epoch 451/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0564 - acc: 0.9833 - val_loss: 0.0389 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.00885\n",
      "Epoch 452/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0695 - acc: 0.9822 - val_loss: 0.0178 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.00885\n",
      "Epoch 453/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0634 - acc: 0.9762 - val_loss: 0.0279 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.00885\n",
      "Epoch 454/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0451 - acc: 0.9860 - val_loss: 0.0205 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.00885\n",
      "Epoch 455/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0722 - acc: 0.9761 - val_loss: 0.0197 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.00885\n",
      "Epoch 456/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0598 - acc: 0.9794 - val_loss: 0.0328 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.00885\n",
      "Epoch 457/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0652 - acc: 0.9743 - val_loss: 0.0265 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.00885\n",
      "Epoch 458/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0816 - acc: 0.9720 - val_loss: 0.0234 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.00885\n",
      "Epoch 459/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0478 - acc: 0.9866 - val_loss: 0.0155 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.00885\n",
      "Epoch 460/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0484 - acc: 0.9835 - val_loss: 0.0178 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.00885\n",
      "Epoch 461/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0467 - acc: 0.9838 - val_loss: 0.0805 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.00885\n",
      "Epoch 462/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0533 - acc: 0.9841 - val_loss: 0.0382 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.00885\n",
      "Epoch 463/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0687 - acc: 0.9756 - val_loss: 0.0136 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.00885\n",
      "Epoch 464/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0733 - acc: 0.9731 - val_loss: 0.0115 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.00885\n",
      "Epoch 465/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0622 - acc: 0.9792 - val_loss: 0.0154 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.00885\n",
      "Epoch 466/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0474 - acc: 0.9860 - val_loss: 0.0178 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.00885\n",
      "Epoch 467/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0491 - acc: 0.9816 - val_loss: 0.0105 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.00885\n",
      "Epoch 468/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0620 - acc: 0.9795 - val_loss: 0.0208 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.00885\n",
      "Epoch 469/500\n",
      "19/19 [==============================] - 65s 3s/step - loss: 0.0608 - acc: 0.9828 - val_loss: 0.0250 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.00885\n",
      "Epoch 470/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0444 - acc: 0.9863 - val_loss: 0.0130 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.00885\n",
      "Epoch 471/500\n",
      "19/19 [==============================] - 66s 3s/step - loss: 0.0514 - acc: 0.9825 - val_loss: 0.0244 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.00885\n",
      "Epoch 00471: early stopping\n",
      "finish 500 epochs!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Activation,Dropout,Flatten,Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard,EarlyStopping,History\n",
    "\n",
    "import pickle\n",
    "\n",
    "IMG_W = 224 #定义裁剪的图片宽度\n",
    "IMG_H = 224 #定义裁剪的图片高度\n",
    "CLASS = 4 #图片的分类数\n",
    "EPOCHS = 100 #迭代周期\n",
    "BATCH_SIZE = 64 #批次大小\n",
    "TRAIN_PATH = 'gpr_data/train' #训练集存放路径\n",
    "TEST_PATH = 'gpr_data/val' #验证集存放路径\n",
    "SAVE_PATH = 'gpr_classify.h5' #模型保存路径\n",
    "LOG_PATH = 'gpr_log/gpr_classify'\n",
    "LEARNING_RATE = 1e-4 #学习率\n",
    "DROPOUT_RATE = 0.2 #抗拟合，不工作的神经网络百分比\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=LOG_PATH,write_images=True)\n",
    "checkpoint = ModelCheckpoint(filepath=SAVE_PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=1,mode='min')\n",
    "history = History()\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # rotation_range = 40,     # 随机旋转度数\n",
    "    width_shift_range = 0.2, # 随机水平平移\n",
    "    height_shift_range = 0.2,# 随机竖直平移\n",
    "    rescale = 1/255,         # 数据归一化\n",
    "    # shear_range = 20,       # 随机错切变换\n",
    "    zoom_range = 0.2,        # 随机放大\n",
    "    horizontal_flip = True,  # 水平翻转\n",
    "    fill_mode = 'nearest',   # 填充方式\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1/255,         # 数据归一化\n",
    ")\n",
    "\n",
    "model = Sequential() #创建一个神经网络对象\n",
    "\n",
    "#添加一个卷积层，传入固定宽高单通道的图片，以32种不同的卷积核构建32张特征图，\n",
    "# 卷积核大小为3*3，构建特征图比例和原图相同，激活函数为relu函数。\n",
    "model.add(Conv2D(input_shape=(IMG_W,IMG_H,1),filters=32,kernel_size=3,padding='same',activation='relu'))\n",
    "#再次构建一个卷积层\n",
    "model.add(Conv2D(filters=32,kernel_size=3,padding='same',activation='relu'))\n",
    "#构建一个池化层，提取特征，池化层的池化窗口为2*2，步长为2。\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))\n",
    "#继续构建卷积层和池化层，区别是卷积核数量为64。\n",
    "model.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))\n",
    "#继续构建卷积层和池化层，区别是卷积核数量为128。\n",
    "model.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "model.add(Flatten()) #数据扁平化\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(128,activation='relu')) #构建一个具有128个神经元的全连接层\n",
    "model.add(Dense(64,activation='relu')) #构建一个具有64个神经元的全连接层\n",
    "model.add(Dropout(DROPOUT_RATE)) #加入dropout，防止过拟合。\n",
    "model.add(Dense(CLASS,activation='softmax')) #输出层，一共4个神经元，对应4个分类\n",
    "\n",
    "adam = Adam(lr=LEARNING_RATE) #创建Adam优化器\n",
    "\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy']) #使用交叉熵代价函数，adam优化器优化模型，并提取准确率\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory( #设置训练集迭代器\n",
    "    TRAIN_PATH, #训练集存放路径\n",
    "    target_size=(IMG_W,IMG_H), #训练集图片尺寸\n",
    "    color_mode='grayscale',\n",
    "    batch_size=BATCH_SIZE #训练集批次\n",
    "    )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory( #设置验证集迭代器\n",
    "    TEST_PATH, #验证集存放路径\n",
    "    target_size=(IMG_W,IMG_H), #验证集图片尺寸\n",
    "    color_mode='grayscale',\n",
    "    batch_size=BATCH_SIZE, #验证集批次\n",
    "    )\n",
    "\n",
    "print(train_generator.class_indices) #打印迭代器分类\n",
    "\n",
    "try:\n",
    "    model = load_model('{}'.format(SAVE_PATH))  #尝试读取训练好的模型，再次训练\n",
    "    print('模型加载成功，继续训练中......')\n",
    "except:\n",
    "    print('未找到模型，正在开始训练......') #如果没有训练过的模型，则从头开始训练\n",
    "\n",
    "model.fit_generator( #模型拟合\n",
    "                    train_generator,  #训练集迭代器\n",
    "                    steps_per_epoch=len(train_generator), #每个周期需要迭代多少步（图片总量/批次大小=11200/64=175）\n",
    "                    epochs=EPOCHS, #迭代周期\n",
    "                    validation_data=test_generator, #测试集迭代器\n",
    "                    validation_steps=len(test_generator), #测试集迭代多少步\n",
    "                    shuffle=True,\n",
    "                    # callbacks=[tensorboard, checkpoint, early_stopping]\n",
    "                    callbacks=[tensorboard,checkpoint,early_stopping,history]\n",
    "                    )\n",
    "\n",
    "#model.save('{}.h5'.format(SAVE_PATH)) #保存模型\n",
    "with open('history_gpr.pickle','wb') as f:\n",
    "  pickle.dump(history,f)\n",
    "print('finish {} epochs!'.format(EPOCHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "maql9YqO1Dm1",
    "outputId": "dd3c1b0f-3207-4fe4-d84a-f4e8e490e08f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "colab_type": "code",
    "id": "8EIPNaTc1JKV",
    "outputId": "70255d2d-099d-4073-f8c4-88bb2f4e18d6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXdgW9XZhx9tyRq25b3j2LGd2JnO\nDpBBEieEWXYLfNCyymoLpWW0QKGljLbQUqCFskfZM5AEAtmT7MRxhuN4L9mWhyxZ835/XOlKiu0E\nKCYE7vOXdXXu1dGVrN95x3lfhSAIAjIyMjIyMjInDMrjPQEZGRkZGRmZr4Ys3jIyMjIyMicYsnjL\nyMjIyMicYMjiLSMjIyMjc4Ihi7eMjIyMjMwJhizeMjIyMjIyJxiyeMvIfI+48847eeyxx4465p13\n3uHyyy//diYkIyMzJMjiLSMjIyMjc4Ihi7eMzHGivr6ek046iaeffpqysjLKysrYsWMHV199NSef\nfDK33367NHbJkiWcfvrpLFiwgMsuu4za2loA7HY7P/3pT5kzZw5XX301PT090jmVlZVccskllJWV\nccYZZ7B79+5jzunxxx+nrKyMuXPncs0119Dd3Q1AX18fv/nNb5gzZw4LFy7k/fffP+rx2267jSee\neEK6buTjOXPm8M9//pOysjIaGxupqqri4osvZuHChcybN4/FixdL561evZpFixZRVlbGNddcQ2dn\nJzfddBPPPPOMNObAgQNMnToVn8/3lT8DGZkTFVm8ZWSOI3a7naSkJJYtW0ZhYSG/+tWveOCBB/jg\ngw9YvHgxtbW1NDY28vvf/57HH3+cpUuXMmvWLO666y4Ann76aeLj4/n888+56667WLt2LQCBQIDr\nr7+es846i2XLlnHPPfdw3XXXHVXg9uzZwyuvvMLbb7/NJ598gsfj4eWXXwbg2Wefxev18vnnn/Pc\nc89x33330dLSMujxY9HS0sKyZctIT0/noYceYvbs2SxZsoT777+fO++8E6/Xi9Pp5NZbb+WRRx5h\n2bJlZGdn8/e//53TTz89SuA//fRT5s+fj1qt/l8+ChmZEwr52y4jcxzx+XwsWLAAgIKCAgCsVisA\nSUlJtLa2cvjwYaZMmUJOTg4A559/Pg8//DA+n48tW7Zw9dVXA5CZmcnkyZMBqKqqor29nfPOOw+A\n0tJSrFYr27dvH3QuJSUlrFy5Eq1WC8D48eOpq6sDRAv4yiuvBCA1NZVVq1ZhNBoHPX4sZs2aJf39\nxBNPEKrSXFpaitvtxmazUVVVRWpqqnRfbr31VgAEQeD222+nqqqK4cOHs3z5cn77298e8zVlZL5P\nyOItI3McUalU6PV6AJRKJTExMVHP+f1+7HY7FotFOm42mxEEAbvdTldXF2azWXouNK67u5u+vj4W\nLlwoPedwOOjs7Bx0Li6Xiz//+c9s2rQJgK6uLklk7XZ71OuEBHqw48ciNjZW+nvNmjU8+eST2O12\nFAoFgiAQCAT6ve/QogKQ3OvnnXceNptNWrTIyPxQkMVbRuY7TkJCQpTF3NXVhVKpJD4+HovFEhXn\n7ujoICsri+TkZIxGI0uXLu13vXfeeWfA13nhhReorq7mnXfewWg08sgjj0gu8Pj4eOx2uzS2ubmZ\n2NjYQY8rlUoCgUDUnAfC6/Xyy1/+kkcffZSZM2fi8XgYM2bMgK/pcrno6uoiNTWVRYsW8ec//xmz\n2UxZWRlKpRwBlPlhIX/jZWS+48yYMYMtW7ZILuzXXnuNGTNmoFarGTduHMuXLwegtraWrVu3ApCR\nkUFqaqok3h0dHdx88804nc5BX6e9vZ3hw4djNBppaGhg1apV0vg5c+bw3nvvIQgCNpuNs88+G7vd\nPujxpKQk9u3bB0BdXR3btm0b8DVdLhdOp5OSkhJAXEBoNBqcTielpaXYbDZ27doFiO71xx9/HIDp\n06fT2dnJSy+9FOVdkJH5oSBb3jIy33FSU1P54x//yHXXXYfX6yUzM5P77rsPgGuuuYZf/epXzJkz\nh7y8PObPnw+AQqHgb3/7G/fccw+PPvooSqWSK664IsotfyQXXXQRN910E2VlZRQWFnLbbbdx4403\n8vzzz3P55ZdTU1PD7Nmz0ev1/Pa3vyU9PX3Q4xdccAE33HAD8+fPZ9SoUZSVlQ34mhaLhSuvvJKz\nzz6bhIQEfv7znzN37lyuvfZaFi9ezGOPPSbFunNycnjggQcAMaSwYMECPvvsM0pLS7/J2y0jc0Kg\nkPt5y8jInIg8/fTT2O12fvOb3xzvqcjIfOvIbnMZGZkTjo6ODt544w0uvvji4z0VGZnjgizeMjIy\nJxSvvfYa5557LldddRVZWVnHezoyMscF2W0uIyMjIyNzgiFb3jIyMjIyMicYsnjLyMjIyMicYJww\nW8Vstp5jD/oKxMfHYLcPvudV5usj39uhQb6vQ4d8b4cG+b7+7yQlmQc8/oO1vNVq1fGewvcW+d4O\nDfJ9HTrkezs0yPd16PjBireMjIyMjMyJiizeMjIyMjIyJxiyeMvIyMjIyJxgyOItIyMjIyNzgiGL\nt4yMjIyMzAmGLN4yMjIyMjInGLJ4y8jIyMjInGDI4i0jIyMjI3OCMaTifeDAAebOncvLL7/c77n1\n69dz3nnnceGFF/L4448P5TRkZGRkZGS+VwyZeDudTu677z6mTZs24PN//OMfeeyxx/jvf//LunXr\nqKysHKqpyMjIyMjIfK8YMvHWarU8/fTTJCcn93uurq6O2NhY0tLSUCqVzJw5kw0bNgzVVGRkZL6j\nBIQAXzRvx+33HLfX7vE4vtJ5fT43K+vWsbl5GwEhIB33B/xsbt6GN+Ab9NwWp42KjgMA7LSV0+nu\n6jdGEAQ2N2+T5tXoaJbO+V+p72nkk5oVHO6qZVvrLro9R+8Z0eZqp7x9f9Qxp9fJ1pYdNDqaOWiv\nOur5Wxp20eXuPua8GhxN7Os4iCAIbGzawoq6tfT5+mjpbaWi4wB72ipo6m0Z8Nw9bRV09NlxeHrZ\n0rwdf8Dfb0ynu4vdbXuPOQ8AX8DHpqat+Ab5HDvdXeyw7UEQBMrb99HoaAagy93NlpYdfFtdtoes\nMYlarUatHvjyNpsNq9UqPbZardTV1R31evHxMd94ndzBCr7L/O/I93Zo+L7d14/2f8YLe99i5rCp\nXD/l/4461tbbjkapxt7XTaYlFY1K86Veo7K9mh6Pg5GJ+dR1NzEiIZemnlae3/4G25vKmZM7nWsz\nLiUpyUxbbwcKhQKrIY7dLftw+fqI18dSkDhcut7Sg1t58+D7APQqurmg5AwAXt31Hu9VLMMe6OCS\nsecMOJdb3v49fT43d836JU/tfoHxacXcfsoNuLx9tLvsZFrS2NVcwQt7XyNGY+Dpsx7kvs0v0eyw\ncccpNzIubdSA1xUEgfLWA/gCfsakFqFUDGyXPbT1TWq6GqTHxckF3D37V4Peu+tf/w0Az579F9qc\nHSQZE3hu87tsadgpjXnx3EfRq3VR57U77dR1NfLQ2icxa43cetLPKUjMRalQ0uKwUd1ZD0CMxkBx\ncgF3bXyedqedn4w5h1cq3gXAIXSz4vB6aWGXYIjniTP+hEKhkF5nn62SJ3c9x7C4TMakjuSDfZ+y\nvmUT10y+hExLmjTu3o8epsVh44F5tzHcmhM11263g66+brJi0wF4ftsbfHxwBU6lgwX5M+nzuXH7\nPcTrY1Eqldy99EnanB2cXjiXxfuXA/CP0/7Ak1ufpa6rken5Y4nVD/3/6QnTVeyb7kyTlGT+xjuV\nyYjI93Zo+D7e150NolW3s6niqO8tIAS4ccXvpMcLcuZwRt6CqDG9XicGtR4Al68PoyaGLnc3d657\nCIGwNfSbiTfy710v0OURLcKtDbsBsXPh9Z/fCcDVo/+Pp3a/IJ1z15Rfk2IUvYh7Gw9Jx98uX8KE\nuFK0KjUba3cAUN50EFt6T3AeLjRKDWqlmj6fmz6fG4BXd4jiv6NpL+U1h3l694s09bZww7gr2W8X\nQ4hOr4sle9bQ7LAB8MyW1/h16fXog+/R7XdjUBtw+VzUdNfz2I6nAbhs5IVMSSvtd/9anbYo4QYo\nbz2AzdaDN+Cj1Sm+TrIhEY1Kw562Cmnc+sqdPLPnZRIMVtpc7VHX2Fy5h5EJBVHHrv/8DunvHk8v\nd33+F07NPoUpqaU8vOWxKO/EKRnTaHfaAXhl17vS8aWVK6Ou2e6y8+neDSTHJErHPq1ZBUB1Zz3x\nmngA9rdXceenD/PASb9HrRQlriV4D8vrqzD7rVHX/deu59ndtpeflVxCmjGF9bXbxPdVu5PllWsl\n78iklPHi+3d2AEjCDXDTx3dL78XTo8DW8839nw62YD8u4p2cnExbW5v0uKWlZUD3uoyMzPebPl8f\nANpjWNEdffaox+Ud+6PE297Xye/W388pGdMwaU18fPhTfj/lFro9PVHCDaKbNSTc6cZUGnub6ezr\nxh8Iu8DfP/QxACPihnOws4rq7jpSjMkIgkC9oxGNUsOCYafyYdVSVtStYVXDejxBC9HjFwXa6XVx\n65q7mZJaymWjLqSqq1q6fmXnYQAEBP646a/4BdHV++q+t7FoTdK43e1hAW1x2rh1zT2cOXwBPV4H\nK+rWcuO4q/jXrueixHBd42ampJUiCIJkpa5u2MCbB8QFw/jkMWxv3SWNb3fZefvgB+xsKwegJKGI\nvNhc3q9aIo1ZVv0ZAkI/4QY40HmIQms+giCgUqro9Q5saH1Wu1oKKywYdiomjZE1DRtY3TB4yDRB\nb+Xq0Zext2M/7x9awn/2vDTguDhdLD5BvAdKhRKXz0V1dx3DLFkEItzYDY6mqPP8Ab/kTn9mT3Ri\ndW1PfdTj7bbdmDRG9Co9/zfqQp7a/SIapRqdSkePVwxxnHnEgnIoOS7inZmZicPhoL6+ntTUVFas\nWMFf/vKX4zEVGRmZ40hTrxgvbHW2cee6P3H75F/y713Pk2lK58LCsOu5vqcx6rz6nkacXicxmhgA\nDnaKsddIIVjTsJFMk+gKPXP4Avr8bj6pWSGJ1LikEnLMWbxftYS9rQdJVIQNiBanDY1Sw2m5c/n7\n9qeodzQy1lfMPRsfosfjYJglmyJrPh9Wwae1K494Ty34Aj62toqu5U3NW7ls1IXssO0e8B74BT8G\ntZ4sUwYHOg/R3tdBpimdxt5mdtr2AKBWqqUY7OLDn0ix9pC1HSJGbeBQ12H2dRzkncrFZJkyuHTU\nBaxv3CyNmZN1EqOshey3H2RLyw62tuxgV9teEg0JKIDy9v2Ut+9Hq9Ji0Zppc7VT52hEgaLfQghg\nW+su1jVswhPwcHHhucTrYwd8nwA9Hgc55izOGF4GQGpMMv/c+Z9+42LUBpw+F6MSCsk0p5NuSgUB\nOj39cwRW1a+n29NDt9uBUqHkiuIf88yel3lk25MATE2dKI2tjxDvTU1bebHidQAS9VaKE4vYHYyf\nh0jUWzkzbyHPlr+CL+Cj093FjPTJjEkq5qclP0GjVLPfXsmKurVkmtIxqA2DvvdvmiET7z179vDg\ngw/S0NCAWq1m2bJlzJkzh8zMTObNm8c999zDLbfcAsBpp51Gbm7uUE1FRuYHRXV3Le8cXMxVoy/D\nHGHFDRUfVi3D4e3l4sIfRR1fVv05LU4bl468AIVCQVVXNe9WfsyVJZcQq7PQ5uqgKyJhqtPdxW7b\nXqq6aqjqqqGup5EzhpdRaM2n3hEt3gIClZ2HGZNUDDBgMtPK+nXS3zmWLIqsI9jTViFZXzHqGHJj\nswGosteiMkbHbeN1sWSbMwFxsdDU2yIlkcXpYskyZaBTaXH7PcxIn4wgQJ2jgbqeBpp6W9nUtFW6\n1q9X343L5yIlJolhlmw2NYvPKRVKAkKAvNhckmMSOdApuuRHxA8nIARoDC5uTkqfIr2fYZYsqrpq\nBvwsLhl5AU/tfkES9QZHE+cXnEWrU/R0zsycwTBLNsNjh5FksLKlZQcfHl6GgMC87JkoFUpe2fcW\nSoWSm8ZdhTfg5e/bnwJgTOIomp02WpytxOvimJAyhj1t+2hxtkqvHxLDSDL1uUzPHM8ble+Ij83p\n0nOF1nzp75KEIva07wPgxnFXsaZhA2cGRV6pUDJ/2OwB33OXu5sdtj00OVswa4yMiBse9fzG5i3S\n3/U9DfT5+nhsx3+o7q6Vjk+Jn8lpBdNYmDOX53e+xz6H6JkoSiigo9aKWRlHT6ATgPzg9ZMZDgFY\nOGwYHb0OlLZ8uno9xBq1A87zm2bIxLukpISXXhrYxQEwadIkXn+9/wctI/NDZqetnKbeZhYMO/Vr\nX+ORrU/iE/ysbtjAotx53+DsBmZp9WcAzEifzJr6jZyafTKpxhQ2NW+lxWljZuZ0cixZvLj3dWyu\ndj46/CkXF/6IVyre7Het/fZwPPlwdw1LqpdTaM2n7gjLG0Rr2xPw0tzb0k/c9SodfUH3NYjuV4As\nc4YkiDEaAxlBy7zaXkeSWrS852XPoqannlmZ09Gr9SQZEjjQeYjX94fjsbOzTkKlVDE3eyatzjYu\nLDgHlVLF5uZtvLD3NdY3bo4SB5fPhVlr4sqSS9nbEc7evmHslSw+vIwfjTidAxHvPcuUQa/XKc11\nZGwx3Z4etrXuotfrinqvkRbx2KRizh9xFivr12ILurhX1K3FG/AyM3MGFxScJZ1nDd6TkBU/OrEY\nnUrL1padlKaMJTc2R4qDgyi6Z+Yt4NV9b/OTovNwdOmoFJoAUbxPypjKbtteKSQhfU6rC+hK8EKe\n+DhZnyI9p1QouWzkhWy37WZC8hhJvK2aFNL7pqNV6jkaq3Y0sG2fA2UyePwedAEzn25sxeopoLXD\nDRo36gTxHhbGj2C//SDLalZIn43PloFC52JVpRezV/QubP88nVElqWgzqliQPZdb/r4F7UhQBUPP\nHqcGry/AXc+K3oyHrp1GzRfDaWp3EHAd5Oozi48652+KEyZhTUbmh0AoSWpO1inHjAMPhi8YP9Wr\ndMcY+b9hc7aztXWH9PivW5/AF/CxvmkzZ+edJm1D2ti0hRxLlhR/Xde4iQR9PAc6DzEibjgLh83l\nHztE666iI3pb0sHOKhocTZJFGskB+yE+r1sjPY7XxXFm3gLePvghvxh/Ddtbd/FxtZhUFHLlWrTh\n5B+jOoYYjYEEfTzVnXWMsIhWYI4li7PzT5PGjUsazae1K6kLLhCuHXM5+XGip/C0IxZHY5NK0Kt0\nrG5YD4jJX60u0er90/Q7USlVtPd1SOPzY4dzZspPSDLE0hWxbSzTnI5GpWFLyw6UPgN/e6GKJ355\nEU29Lf28DCdlTGVNRLhgVtYMZmXNYEfrbp7e8xKLDy8DRMs5kjidRbL8DWo9Fq2JulYHqZ2zSU9L\n5rOt9WzY1wDBpG1DIAGT0srNpdfh8fq5/cVVqDN9aIKG9MyM6VxUcA4vVbzBrrZyXMF8BlDQ2u7F\nEBTvt5faOPUagermHrKSTUxJK2VKWil1PeFkulc/PcDGvS00tfdyamkmKfFieKS108WqHQ3MHJuO\ny+3nhaX7UafqpD3PnZ2weH81IFrHCmMXKks7Ka5J1NapIOsgn9eK35lTrWeyeLOYp2DDx3Mf75Ne\nf+8eJc8suormjmD83hf+X6yodGLwhmP/v/mXeO/VKiUb97Zw4Zx8Yk1D+78HsnjLyHwncfn6vpZ4\n72gNx1UHik+CGF/e3baXHEuWJEJfhYAQYHvrbl7f/y69vnByUuS+2PeCCV8Am5u3M8ySTWfEft8P\nqpYCMMySTaE1n3un3cZdGx7A4e3t93pP7nxOSgYLoVGq+1nbw2NzmJw6gcmpEwDEawXFO5R1bNGF\nxTtGI8YnM80Z7LTtoaZb3K4ap4uO2Z6dLy5EQq7ukBU/8M1RMTFlHGsbNwGiuL+w9zXJUgdIjkmS\nhq/a2cjLnxxg/qQs4q3hzys1JpkMUxpjEoq5+uGVgIKNe1swa0w0ERZvBQryteNZwwaSvCN55qO9\n7D7Uzh2XljIiPk/yQMzOOoki6wjpPJfbx5srDxGjN+Hwd+N26Glsd3L/S1vx+AIs2RT2GhiC4v3q\nh828JfRw68Xj2XZAtMiFPqM0rqFRIDlP4LJRFwZ3B9yGVZFJSJLN7hx6dDW4u42s3tnIC0v3M604\nleklqWw/aOPMU8QQRro6j91Vojgu31LPyu0N3H5JKSu2N7C5ogWPN8CnX9QRykMTPGGhVPp1/Pzs\nEp58T8wVEHpj6dt+KmKQQUCXFINPL35nO9sMgIeTx6TR4/RS09KDvSfsrfnlY2s5tVQMm6TGxmFD\nfM8Hq1309YihF6NeTW+fj6xkE9eeVUxFjR3Lie42l5GREeNxfsGPVR9/1HEBIRAlRi6fk1jdV9sr\nWtVVzdMR2bg2Zxvtrg4SDNFi827lR+xqK0en0vLwyX+gwdFEmjEFjUpDj8eBx+/pd04kG5u28Mq+\nt770vPr8fQPGQgFSgkJW1+AHQQGKsIBlmzOp7anH7hZjjQXx+RwIbqMqjM+XXKwAGqWGH404Pera\nuRZRDBIi7n2k5R1KdssypbPTtoedNjGRLU5nwe31s2p7AyOy4kQLMTFDEu/Iz9Lj9fPC0n24vQFS\n4g0s2VTL9edNZZNyKyqFinHJoylJHIlOFf5BTwyKv1lrYs1OUQQ++aIOEDBMBgVKGttcKJUKfL4A\nIHosXly6H22eC1WCeB1vbSHnFM/miderQDmX2oCKWkQX8Yfrq/nZolHcMvZmFCofaebwggHgpU/2\ns7G8Bd1YD0odeHr1PPjKNjy+AEciBBQolAL+Ph0u/Nz7vBhDtsRoUKniCdnXT75TwZi8VsbmJVAy\nPIFR3T9m6z7R66BWKWjdWQjKAgioeHe1mGC4obyZfbV27D1uPt/WAIp5HBIUQHgh6PML3PdCOG6d\nnWLC6wvQ3eth7sQsDrRDNWKMOt5gZlJRMvGXlmKzu3h68V5y08ycPDYdnUbFKzurEVL3o1fpaGoU\nUCoU/HheATqNih2VbfzjrXAWfo/Ty3trxF0BqbFxhHYytnX4aetoIzvZxF2XT2L7wTbyM2OJNWpJ\nSwgvZoYaWbxlZIaQP3/xKD0eB4/O/NNRi4q8XPGmJA4ATsnlGI3X70WhUKBWqhEEgTZXB37BR6wu\nlhZnW9TYtY2bWNu4iX/M+rNk9QHYg9m0br+HV/e/zcamLZyeW8aCYXO4e8MDuP2efudE0tzbOuDx\ngSiIy+dAZ7j0cUnCSDJMaSyr+RyAFGMSgiDwj7d2o58ICgWYNEZunvBzYnWxvLb/Xb5o2cb8nNl4\n/B5JvKenT5HE+/Tc+UxNm0hLS4BDLhtj8qxo1Co0Kg33TrsNFWreWFHJyWPSosVbbUAQBPKC3gdv\nwItSocSiNfPf5ZWimAQ5b1FYsPVqHQ02BxvKRbfu9oPR9/3p9w6Smb6IKxaNQK1QoVapogqLfFFh\nY77x/5hYkM5da7dHnKmgb8dMhICKuzdvjrqmWqXE5w8Q8GgJfSqBPiPvrgxayIHon/KN5S2UTcrm\nrme3M3lkMteeJYr3rkNtrN/TzOYK8TNUhBZLghKHy4tWo+S2n0zgmY8qOPukXGydfbyxxg0KAZNB\nS6/LK/lzbr+klOU7qljPGoSAInj9dnYditxOpsSgUzN+RCLr9zRDcG3Q7fRKIyKtXZ1Gy8jseHZU\ntnH5wiKGp1n4aGMNe6s7mFSUzEWnjkCtii5A89ZKH9XB6yZbxM83PyOW/IxYUhNiyEwyogkW+Fq/\nfxSHhP2kxaRT2ewgPdGITiM+NyYvgQtm52PUq/lkSx0d3W5cbnERkWSygJRbKb7XOaWZKJUKSguj\nF0bfFrJ4y8gMER6/V8pO3t1ewYTkMQOOCwiBKOEGMcHpSARB4Jer7iTLnMFtk37B6oYNvHHgPUCM\n905MGTfg9e3uTqz6eKnqVmSG98Ym0aKp6NjPqIQCqZpVbU89wyzZUaITIhRTj8SiNQ9YajNFm8Hu\n/bFoMipRmrpIMiQwKXW8JN7JhiQO1ImWteDRo9C7mGydIRVEOW/EGYxOHMn45NEsrvpEum5WRMZy\nSeIovC4tD766EYBzZw5n0bRhACQYrGzd38rSTbUs3VTLry7Nk85z9MAvXlrLebOHScdGWQvpcnhZ\nuT3aJb+3wgfB2iAut48n3y+nsU108eemmSnOTWDx+moAPL4AVbVuqmp8PLZhI3npFsomZ9Ng6yUz\n2cRTH4r7ij9Z344AnD8rj8a2XtbtaUbwDLzV6NaLx7F6ZyP1tNKK+DqBbiuBiL3pi6blMHdiFp9s\nrmXJplreWytajZsrWrmszMtnW+t5N2hJAtzwo9E8Vf4FKq0bwSVajGfOyGVYqoX7fjZFvEcuL2+s\nEBdMF87PZ/mWempaeogzaUmxxlCclcqKTyYjuA0snJrNko1hd3sIl9vH+bPzCX2T9tbYowQboDAr\njmklqaTEGyjIiqPT4SHeLLrDrzlGAlhaggl/ZQKq2HZyUqJDHrlplqjHeUkpVOyaTIVHh+ALkJsW\nXswpFQoWTBG9NSePTcfe4+aWx8UM/2RzLAR3mV15+kjKD3cwZVQKxxNZvGVkhojG3vCe0s3NWwcU\n7y53D79b/6d+x11eF61OGw9+8Q+uKP4xJYkjcQazjEOJPbXdYhGJREMCba52qfDG9LRJrG/6QrrW\n3RseRIGCCwrO5qSMKfR4HNI5ChSYNEaqu+tY27BROmdry06e2v0ic7JOZl7OrKi5HVkwBcTY9a7g\n/ulIfG4Nga4k3D3xnHeegllZM9BFJNKZtEYO1ouxRM+hsSiNXXy0WcFZRQHUKiUmrZHSlLEAtNvD\n1tq/3tnHVaf8nE6hkUxTGrurwklgb6+qoqLGzo0/GoNOqwonHQGPvX4ATXCN88anNThcSj7d3BjK\nb2Jy6ng2lDcTEATG5SdS1dSN2+Nnb1UP5xSfhQ4T1z+yOuo9nn3ycIpzraTEG3jmo3BRlWWb62i1\nu2i1u9hQ3n8rW8iqy041UzYlm8sWFLF8ax1qlZL2rj6mFqfQ2eNhb00HeemxjMiM47PaNt6p3IZR\nE0Ph8FR2VLYxLj+Rn59dglqlQKFQkJMqClIoLg3wxopKVu8Mfx8nj0xmQkESng/Gok6qR9WRR9m0\nbEm8pM/HoOGKhUV0Oz1ML0nGm8KtAAAgAElEQVRl3e7gNju96EUakRVLoMdKijWG82fls2hqDjc8\nKiaETS1OYWN5CwkWPbFGLT87XUyYa7A5+Ptbuzh5bLrkPh+RFccpY8MLspBwfxlG5sRjWDOZorQu\nFgyfedSxOSlmAj3hkNCEgsGt5nizjvuvnorL7aNJCCdSTi9JY3pJ2qDnfVvI4i0j8w3wwaGlBIQA\nZ+efxgH7IT6sWhaVDBZqXrDTtofPalczIm44vT4XBfF5Uc0tQjh9fXxWu5o+v5sX9r7Gw6f8QSoh\nCWL1rva+DhQomJ89i1f3v01bXwc6lZYfF50XJd4gJq8d7DzEuOQSBASyTOlMThlPrM6CzdXO8tpV\nUeesadiAT/Dz3qGP2dqyg8uLf0xq0BqOFO84XSzT0iaiVCgHFO8+V9DJG1AzzjJFijNfUHA2aoX4\nXKdDtMKE3jj8vXEAXP3wSi6ck0/ZZFFM3B4/63e3oQ2Wpa6s6+WzdV386vzpNHc42VcTvaDYW23n\nuSUV1LQ4aAmK97j8RHZU2ggFL9rsfkCJANw16xesqvyCAvNI3tq9HbVKyZWnjyRGr2HT3hb+/UE5\n737oBsIW48Kp2cQZdZTkWlEoFEwvSeXFZfvxBuPGIcscoLQgia1BMS3MisMUo2HrfvFxVrIJpUKB\nUq1g4ZToutukwrgR4XKg09Im0uBoYsGwU4lRxNLe1UdWinh+iOyU/rkSodj6ebPyWDAlWxqv8Gvx\nNQ1n1sQMzp2Z1+88EK3QEHkZseyr7aR4mCiARr2GP189FXOMeFdj9BrG5CVQfriDy8oKKclPIicp\nOg6ckWTioZ9PB+D9NYcJCALWryDWR2K16HnkuoH3gB9JaGED8OhNJ2GJOXpyWapV/L42NX47zUa+\nCrJ4y8h8A4TcwNPSJ/HMnpdxeHulLT0apQa7u4tWp42ndr8IwKFgqUx1MK587ogzePvgh9L1XD6X\nVNYx5O7ucIW3ErW6bLT32YnVWSiyhutKp8QkDejqBqjpruPJnc8CYtb1ouHzAThgr2R5rVgjujih\niHZXB80RhTfqHI38Z89L3DrxRhQBFU09baAQs7sTu6bz2ce9nHNW+KdkeGyOVETE0R2ey3+XH+DG\nc8eg1aiYmTldOt7pGLij2OufVzJjdBomg4YD9Z3gj4jBC0rKD3dw5UMros45tTSTz7aKHolQXDfE\nNWcW8+sn1iEtlfzButcdTgoTCkhRZvDAK9to6XBy0ug0ybosLUwi1qSlKzjP4lwrV50+ql9WsUKh\nQK1S4D2iGdVTt85CrVLy19d3UH64g0XTc6htcUjifSwBiSRGE8Nloy6UHpsM/fMokuPDrvdQNrSA\nGE44bWr04iA7xUxNcw8m/Zfb2XDWSbkkxRmYVpwqHUsJClyI688pweMLoNeqOeuUvKPWrL/zslKW\nbKz51lzQ8WYdc0szyUw2faX7PiF5LNtadzE3++iW/beJLN4yMl8BQRDY0LSFVGMylZ1VnJwxFaUi\nLCp/3/YvabtTKG5dZM1nd1sFaxs29bveLpsY/yyMz4867vL1SfWuHd5ePq1ZSWJcOH7X5Gih093F\nMEs2CYZ48mJzOdR1mOKEokHn3t5npz1oNUcmbuVHVKTKMWeSoI+XxPvhk+9h8eFPWVW/jid3PovS\nYyag8OK3J3HLnOv56QPiosXlDFtOF2T9Hw903QuAvTNAqPNwebWd1Tsb8QcEDjV0cc1ZxaiUSroc\nblRKBWfOGMa2A200tPXi84sSu2ZnIwun5rC3ugPBf+yfq/mTsjh5TBr3PPdFv+d0WhXzJ2WxuDEV\npc4FKMjLsHCooZtGmwO1EOBgXSc5qWYuW1AonadWKbn9klL8/gA9Ti+ZSSZi9APPxR8QF1ynjE0j\nKc5AgkUvJVhdc2Yx9a0OinLi8fmHzpJTKhTMHp+BrdNFfkasFPueVNS/f8SNPxrN0k21lB3hLh8M\ntUoZ5d4eCI1aJSWIHYvcNAvXnTP6S439JlAEs8u/Knq1jhvGXTkEM/r6yOItI/MVqHc08sq+cGWw\nys7DnDviDOlxl6cHk8YYtV85NSaF3VQMWGikva+DeF1c1N5fAKfPFVHkInrfNMDLwTmE9hz/asK1\n+AW/tJ/5SOJ1cdKWK4gWb483wPysuXxSt5wJKWNp6m1hdcMGrPp4YjQxnJO/iLqeeql+OEDAacHj\nDSeutbWH49F3P7uZolNGUNN3kJYWgVRrDCMyY1mzq4malh7W7RZDCOV/X8tVp4+i0+EmzqTljBm5\nnDEjl/auPvyBAHc8tYltB2wsnJpD+eEOVF/i5yrerIuKlxbnWik/HI6HnzEjl6L6n/Lnl8XOUZMK\nkznU0E1NczdGjehCz02z9MtoTo4Trdm0hKO/vj8oygadWkqaC2EyaCjKEbPWR+XEMzYvgRmjhyZ2\nemmZuPhYvTOceJccH9NvnNWi/1piJnP8Gbjpq4zM95SAEOjXWWiw4y1Om9T1KsSGpi1Rj6u6quns\nC7uzE/VWzs4LV+dSKpQkxYi/+JEVpCLJNKejOUJ0XT6X1IrwaIQqh4W2jw1GKF4doq1dtGwb2nq5\n8dE1vP+2ikvTbyTNmMKIuOFolGoyY7L4x1u7sHW4+fnoq1AemEXf7hn07ToJX0M+978czpC3tYo/\nJQGHOJ+cvplM9FyKx61i9oQMLltQiFqlpLbFEX6Pbh8vfbKfTocnqiJVQqye5PgYCrJiqWrs5mB9\nJ/W2XoYlh7drzQ0Wz7jqjFFce1Y4G1mtUkYJ749OGc6s8RnccmE4E39EZhynT8/h0rJCyeXb0uGk\nqV2Mjacl9Be5L0soRhp3jApbWo2KX5w/lokDWMPfJKH3MibvGKsOmRMOWbxlflAsr13F/Zsf6dfY\n4dOaldy/+RGpmUSnu4t7Nz7M4zufkcZ4Az62NG+POs/l65MaM8zJOplfT7whqhmIWqE6ekUuGLDK\nmcvXN6h4/2L81Zg14mskGQb+UQ695rVjLudPM+6MsrQB3l/ZSLfTw9Z9rUFXr4Kl65sQBAGz1sTt\nk36JorGEHZVtPPneHjaX2+jt1CO4zGj9sYAiSogPHnaT3LIA975JADS1u1i/q41Uawyzx2egUirJ\nSDJS1yqek5NqJiPRiL3HjT8gDCh24/ITEYBng2Uri3PC3onzZ+dx208mMK04lZwBErTOn5XHyJx4\nslNMXFZWSHFu9Gfwo1PymD0+Q7LSO7r6pASz9MSvX2jjF+eN4bSpOVJlruPNiMw4bvvJBK47u+R4\nT0XmG0YWb5kfDAEhwLpg3Dmy4QLApmbRjRrKmA41iajqqpH2au9pq4gqBxri9eBe68L4fMxaU5QF\nrFFqosQ7Jab/1pQjuyCB2Dox9LoAyTHhjOOC+HxKAxfgOTQGX5vodv3kizpW7Qhb9reUXsd1Y3/K\n6MRRxOliMWnDguQ+OB6hNw5bp4udh9pRKhSMHp5AbauDv76+g301duI0CZRXiolGDW29vLhsP0qF\ngtsvmcDfbpgRZcmW5Fpxuf3U1IBJJ7qXdx1qxx8QOGP6MMkSzkoKL2rG5iVwyfywu3agkpKhLOuW\nDicqpSJKvDVqFQVZYmZ6UryBGaNTuWJhON6/cGoOt148HpXy6D9xIfFu63KFxft/qJKVGGfgvFl5\n/dzux5OCrDi0mi8Xg5Y5cZBj3jLfO+p6Gvnnjqf5+dgrGGYJJ+Ic6qymLdgUYqAa2pEcjOjw9Mye\nl6l3NJJsEMUjUW+VrhNJvF4UE40ynLmrVqpJMMRj1MTQ63Vi1ppoOWLhkGXO6Hetxt7mqNrkFq2Z\nn5ZeQJu9h06HmyUbGoF01uxsYUpROm98LhbSyMuIJTPJRKzOQqwuokBF8FIqhYqAXczsPVjXRXVT\nNwVZcfxs0Uie/rCc8mo7FdV2NGqlVCpTo1ZSlB3PvImZjMgU32NxrpX8jFgqG7q48NQRLHS4qbf1\nMrEomdv/vQGPT9ynPWlk2C08IjOWtcF9wqnWGAqy4kiON9Bqd2HQ9heXyBjtmLwEzIaBXdFKhYKf\nLRo14HPHwmTQiA0l9jSjVChIjNUTZ/p2alPLyPwvyOIt873jvcqPcHh7eX3/e/x20k0EhADPlr8q\nFTEBoqzagTgQkZwVStSq6alDr9KTF5dLW3N/8Q6JpTqirKhGqUapUFIUP4KtrTulQisAoxIKGZtY\nLG0Fi+TIvd8mjZEJ6aOxaXp47bOD0vFDDd0s3lAtbSv704tbiTNpEYDfXDweq0VsqRhaCOhUOkLv\n/O1VhxCAyaNSsBi13HzhOP7+1i52HWqXhPven04mM3ngnuC3XDSOBlsvGYlGMhKNjAzu/bUYtbR1\n9ZGVbIyyQKcWp/DcEtEFnhwfg0Kh4HeXTWTpplrmTOi/gAFxa9L7aw9TNjmbJEMsp2RMY1RC4YBj\nvw4KhYI4kzjfgCDws0UjB91qJyPzXeK749uROaHwB/y8eeD9fu0Jj0VHn52XK96UxFMQBD6q+iTK\n0v1fCbmtQ12uer3OKOEGcHhEyzsgBHincrEUt/YEvPT53LS52qMaWoQwqPVRMe1cSzanZExnQvIY\njOqYqNcX35+S6uZuzh1xBnmxw6Iy0xflzuOkjKnS46tGX8YoayHzc8IFJy4u/BG5lmzpPJfbx8rt\nDcSbdfx4rtgl6qMNYvzeZNDg9vppCVb1qmoMd/Gamz2LYZZszsm4SDrmDwjotSqmFYuWuEKhYNSw\nsIv/ytNHDircADqNiuHpln7HQ5XDjmzSoFGruP6cEkoLk8hOMUlzPm9WnrTIOJLTp+fw0M+nUZAV\nh0Kh4MLCcxid+PWs7MGIC7rO40xaCrOP3kBGRua7gmx5y3wtvmjZzsr6daxr3Myjs/qX9xyMlyre\n5IC9EgXwk5Hn0+bq4OPq5XxcvZzH5zz0jcwtJJ6egLh9qdcbjlOfMbyMD6uW0eMVFw+VnYf5rDZc\n7rLH46DVJbq1cyxZ0r7oEHq1DpMmLEq/nnhDv9ePzBy3dXi4d80Wnr1tDjeXXgfAwmFzWd2wnjRj\natR545JKGJdUwkH7IT6pEYuPTEubFCXwhxq68PgCzCtJ5dTSTEwGjVQr+/pzSnj24wpidBpqWnpw\nuLy8uaKShrZexuYlcODzYioC0TW7Txmbjl4bnm9WRDWsgRLBvgy9faJ4D1QEo7QwmdLCL59hrVIq\nSYwduN73N0VosZFq/fpZ5jIy3zay5S3ztQi5db0BL3vaKhCEL1d0oivY01nM0rZR01N31PG72/Ye\n08V9JKHez16/KN7OYLGUedmzKMuZg1qplizvIzO6u909tPaGxftI9Co9Meqji0mU5R0Q/8Vcbh/1\nrQ7ueGojo2Om8eBJd0ttIrudHp5fUsFLy/bT5/ExLFasgpVnzuP3//lC6m0MUNkgzjcvIxaFQsHU\n4lQWTM5m8shkCrPjeeCaaZw7U0yAq2npYcmmWnYdauelTw5IBUQiCZUfDRFpaad+zS1Tp08fBhAV\n7z4RyEwa3MsgI/NdQ7a8Zb4WqoiqYk/ueo6rSi5lXPKxKyUJQdFXKpTcu/HhQcd1uruo7qrl6T0v\nkW5M5c4pN+PyuXD7PcTpxL3EASFAi9NGmjFcWrHd1UGbSxS7Xp+TXq+T+h7R2ozRGFAoFJg1Jlpd\nNtpcHf2yznu8Dqm6WIYxDbVSLbnfQbS8OUZINDJhDSFY2rS7jyWbamjucPKH578g3qzjj1dOwaBT\n8+bnlazbIxYuyc+IZdLIZOIOn8meNhcITh55Yyf/+e1s+jw+Nu4VwxR5Ee7qC+aEq7MpFApMwTrT\nq3ZEW9mRXLGwiMRYfb8GEOYYLanWGOLNumNmag/G2SflcmppJrEDZJB/F7ly0Sg+2VrPmSf137In\nI/NdRRZvma+MP+CPqv4F0BrRS9oX8CMIwoCJP6GSn5ElRUO4fC4MQav2znVhV3xjryhs9278C92e\nHqnX9Kbmbbxc8QbnjTiT2Vkn0et1cteGByLm4eM3a+6RHodi0iatEXtPJ/dt+ku/rVsBIcDhLrGt\nYXJMIgaVnp5A2PLXq3SYVeLiwaoRz3X2eel0eEhLEJOwVJEJaEHLu73bjVIZvh/2Hjd3PbOZzCQj\nOyP6Hx9u6sZq0dFk8wDhe/TM4r00dYix7JR4A+aj1GWOrHetVCiYVpIiVTX7xy9Oprqpm5Lhgxft\n+NNVUwZ97sugVCpOGOEGcc/5HZdPPmoNbhmZ7xqy21zmK3Pzqt/x5sH3o46FhLrd1cGP37yBD6qW\nDniuP2h5D9Sv+ter76ai/cCgrxvqF+0JiG7xUJLbWwc/APq7wI/EoBEXBqGqab6Ab8Bqa/vsB1Er\n1cTr4zCooxOp9Go9TTUxuA+Oo3GTWPjiiff28Lv/bOKJd/cA8PnWiGsGxbujpw+bPfo9t3f3ScJ9\n7VnFKBUKlm+t58FXowvBZCQZ2VDeQnVTN0a9mkvKjp5tbTaEhdNq0VEQ3N4ForAfTbhB/CzljGsZ\nme82snjL4PK5uG/TX9l4ROnPgXB4e/EJ/n7HQ3HpfR3iNqZQwtXS6s948It/SDHykOU9mNCualh/\nzPi5xy+6sfXqsMu3o88uxbFBTBqbkR5tQYYsb3cwJh5yv6fEJLModx6n586XxuaYM1EqlOj7ibeO\n1k4XAXsqWmUMbZ0u9laLSW07Ktvw+QN8+kXYXR2Kfy/bVMuB+i70WhWXzu9fS3piYbK03etI7rik\nFEvQFX7bJaVSO8bB0GrC/9ZJcQbG5iei16q4YHb+Uc6SkZE5kZDFW4bDXbU097bwUsUbAGxs2sKr\n+94eUEQj3eORhK1ib9TxD6uWUdtTT7tLFLiQiNv7OhmIeF0c3iOuAUTNxRu0vCMT2fZ1VEoZ5AAG\ntYEfF53LWcMXSsdigpb3DPNpmFx5/HL0TSjbc9F1FDDGNI3xyWOlsQXxYm/jUFJZCL1KT3uXaLkn\nWvRsqhBj0EqFAn9AoKndic8XUVwlRhT/lqDVnZtmYfaETDKDWd0Lp2Zz84VjUSoVzJ0oltQsLUhi\narBFosWoxaBTc/cVk3n4xpPJ+BKlOyOt5qQ4PRajlidunsmCL9k5SkZG5ruPHPP+gbOh8Qv2tFdI\njz1+jyTiZ+Yt4OPDy+l2d3P68PmkGlP6VQcLIYm3f+DezK2uNpJiEiTrcqAyowC93l7JMo6kzx+O\nsXuCWeSRlvbBzkPkmMPZ4aGex5FVxmKClvc7S7qAEbztq6H3UCH7gbt2baZscvj8/AFKlkLQ8g4K\nsV6nkmp1l03OYsmmWsoPd+By+wjlo8cbY2iOOD9UtOS3P5mAxxuIShg7d2Yec0szSY6PQRAECrLi\nGBnsQhVv1pGUZP7KcdmkuKHdZiUjI3N8kMX7B87LEe0tgaiGHYc6q1lVvw4AqyGeH+Wf3i87O0RI\nvJ0DxLJBrCVenFAouc0H41BXNWsaNvQ7HmllhyzzHq8Dg1qPSqHioL0Ka7Coyi/GXyNZzqGuWwBG\njYFuZ3hh8EVFa9RrrNrRSNlZs9nWuovhscPYXNEiVRoLIfjVtNjFhUef2093r3i94lwrSzbV8saK\nyqjxOrWGs07KRaVU0NTuZP4kcYFg1GswHlGXRKdRSSVBFQoFs8YPXHXsq3Cs7lYyMjInJrJ4/4Dx\nB/oL6X57WHzqeuqlv0OifaR4/3P2g/xx018l8Q6JbNR2qeB5lZ2HB7XMb5v0C/654z90urv46PCn\n/Z6PrEUeuobD04tZayItJoWdbeXSlrDIIiqxurB461Q6th8OV4Q7MiigVCg4K28hZ+UtxNbp4l/v\nl6Mt6kYVUURs1/5OBEG0Zp1uHwJg1KsZlhouaBKjU0vXTos3cVbR8duCFKOX/8VlZL6PyDHvHzAD\nWck7beXS35H9p0Ox7ubeaGtVoVBg0Zrp9TrxBXySiGtVoniH9oPvad/HI9ueHHQuOpWuX9vKSCIt\nb0/AS0AI4PD2YtKYSA3u8z7UVQ0QVb40LkK8FQoFdS3RBV+MEeLmdPu49Yn1dDs9Ue0uI9l32EGq\nNYaMJCMut4/uXg8Wo5YYvYbLFxZRkBnLJWXhhDSD9vhYvr88fwwTi5IpyZX7OMvIfB8ZUvG+//77\nufDCC7nooovYtSu6tvTy5cs599xzufjii3n55ZeHchoygxBZNhTEzOhQjW8Qu3MBKFBgc7VLVdGO\nJCSWPR6HJN5HNtboOKLM6JHoVLqohh5HEhnf9vq9OL0uBMTe06G92i6fCwUKjJpwZbDIhDN7j5um\ndvE6uWlmtGolsydE911u7+5jT1U7da0Dx5YFv4qJRcmY9Br6PH4cLq+0p/mUsencdkkpU0eFy55G\nlkr9NhmTl8h1Z5egUcvrcxmZ7yND9suyefNmampqeP311zl06BB33HEHr7/+OgCBQID77ruPd999\nl7i4OK666irmzp1LamrqMa4q801ypHjnx+ayzx7uWNXlEUuZjkwoYG/7fna37UVAIEEfH1Xz2xh0\nUzt9Lqn8aZ/PjS/gO2aMO4ReraP7KGVQIzPJPQEvjuBjk8YY1evaqImh0ebkyff3cNUZoxiWauGW\n0uvocfi45XExfq/TqvjFeWPp7fPS2NY/ce6ZjyoIJbcbFGY8hDuICYKSsXkJ1LeG5zNQL+oQ6uMk\n3jIyMt9vhmxZvmHDBubOnQtAXl4eXV1dOBziD57dbsdisWC1WlEqlUydOpX169cP1VR+8Ly+/12e\n2v1iv+O9R/S0zjSn9xujUWrIixVjtttadwJQGB+9XzgmuBf6se1PS7FpAeGYNckVEXVGtUrNUa1U\nmzNchczj90jXNmuMUVXSTFoT7609TFO7k6eDDTuGxw5DcIULlSTHGbAYtaQlGKPc5iEid8jdPe9y\nCnUTpccJZj25aRYMuvB5AzXgCHG8LG8ZGZnvN0P2y9LW1kZxcbH02Gq1YrPZMJlMWK1Went7qa6u\nJiMjg02bNjF58uSjXi8+Pga1enC36tchKenrdU06EWjobub57W9yfvEiVgezt2OteikWDaDoiXZt\nLxh5Mo2uRlocbdK+7GRTAiNSs6AK9naI1c/GZBaxvukLQLyHie1xUBNtHQM8uPUfR51jktFKa68o\nysnJFm49+Vp+/9nDA24V29S8Vfr7v/vfYXRKEQDpCUnkpIdrm0/JGsum/W4AmjucmCwGDDo1mrpw\nUZiEOIP02SckmDi3uYeWDidrd0bXAl84fRgjclK5L+dnbG2czPrarVxz7rlo1WoSIrZgpaeYB/0u\nxVpivtHv2ff5O3u8ke/t0CDf16HhWzMLIotsKBQKHnjgAe644w7MZjOZmZlHOVPEbh94X/DX5evs\nmT0e+AN+PqxaxtS0iaQav3yXpjtWP4jL18eelv3SsQP1dVFWanOHKJxKhZIf5Z+OwWvh+tFX0eps\n4w8bxfacVq0Vg88szUWpUFIQU8iohELGJ43BZush4A5b0EXxI8iIT+GzqrX0uI9uecdqYmlFnIPN\n1oORWK4dcwV/3/7vfmMFBGakT2Fd4yYAdrfsA0Dt1WOz9TAteTptrjasPSUcbhQtbkGAlZtrmFiU\nTENTWLzbO11Rn/2iKdkEBIFTx2fwh+fFRckl8wuYMyFTGpetGUZ23jC6QiVOI2L6KkEY9Lvk6HF/\nY9+zE+U7eyIi39uhQb6v/zuDLX6GTLyTk5NpawtX42ptbSUpKSwckydP5tVXXwXgr3/9KxkZ//ue\n1u8jaxs38WntSra27uS+6bd/qXNcvj6pcUhkzLnd1REl3qGY968mXMvw2GHS8VAlMoAsUzqJhgQU\nKBAQSDOmoFNpuX7sz6QxhogWmUZNDAZN9AZmjVIzYNW0eH1cv2OGo7TbTD2iiQhAnM6CvcfN5s8S\ncLgs7EIU7pnj0lm1o5GKGjttXX0sXl8tnRNqmRmJUqEgJ2K717F6WUe6zePM8l5qGRmZb5chi3nP\nmDGDZcuWAVBeXk5ycjImU3gLz5VXXkl7eztOp5MVK1Ywbdq0oZrKCU0o+9vp/XKeh8Ndtfx71/MD\nPre1dSdd7vAqOCTeRk10yc3IftWZ5nS0Kg3WoNBmmvrHxWOixNtIzBHinWQIb1d6dOaf0AYzwEO1\nxvUqHQ++so3y6g4pfj4QkXu2pWOaWP6zeC8OV3hxcN6sPH4yrwC9VsWew+28saISp1ush37/1VMZ\nk5fY7zohzp+dR166JUrIB0IbEcIpyu6/CJGRkZEZSobM8p4wYQLFxcVcdNFFKBQK7r77bt555x3M\nZjPz5s3jggsu4Kc//SkKhYKrr74aq/XozRZ+qHQGs7cjy3wOhiAIPLbjKdx+DzqVtl/seGPTFlp6\nbfx64vVAOGEtcmsViG70EJkm0SOSHJNEe599wKS2yM5bJk2MJM4hkgwJUltPjUojFYdRK9U8ePLd\nbN/fwXMbDrL/tR3885bBF3FxA4j3W5/XUVFjJzfNwuEm8V6dOiETtUpJYVZcVLtN4JitKhdOyWHh\nlJyjjgEw6ETxLsqOQ3O0XAy5OZeMjMwQMKQx71//+tdRj4uKiqS/58+fz/z58488ReYIQluvYrXH\nFm9vwCcJ9p2Tb5Z6W5s0RikL/HC3WP7UH/Dj8PaiQBFlOR9JyOJOM6ZQ0XGAbHP//ATDEZZ35J5s\ngERDdKGQkCtfrVRh0hjp7Q0LbI8jOokukoGKuKzf3UJWsolfnj+GT76ow2rRo9OKYlowgHjrtd9M\n0uOUUSkIAkws+vJ5CDIyMjLfFPI+lu84UuvML9FfOVQ2dGxSSVQ8OcOUJpU9zTClsbJ+HW8d+AAB\ngVitOcrSDvHr0hsICAGpQ9W8nFlkmtLJi4iNh4g5IubtVEbv7T5SvEOE9kC3RCQj7qrsGHAswL/e\nq4AjSgFMHZXCFaeNRKNWcu7MvKjn8jL6W+rfVJ9qlVLJjNFpgz6vUarxBnzoVYOHAWRkZGS+LnL5\npe8wXr9XsrzdfveAYw531XLvxodpc7VLVrdWqY0SZIvWwiVF5wOixf3mgfcRgtW343QDx2tzY7PJ\nixsWcQ0zU9JKBxS/SE8CjxcAACAASURBVLe5URPDGYXzmJc9K+rYj4vO5aZxV0edp0RFICDQ0hEW\n773Vg4v34fr+cf/iXOugVcSGHSNuPZT8ZuJNzMuexbikkuM2BxkZme8vsnh/h3F4eyWRjYxfL6v+\nnE9rVgLw/N7/0uK08cGhpVKfa50quimIWWtkWvokEvRWmp3RtcnjvkQs/VioIsqamjRGdGotZ+ef\nJh3TqbTMSJ9CoTW6uMuanc3c+/wXtNhdWC06zDGafm7uSCILngR6Lbj3TcQcoxl0vFajigo5J1i+\nPSs43ZTK2fmnRd0bGRkZmW8K2W3+HSa03QugubeF9yo/ZtHw+XxQtRSAU7NPkWp39/ndYcv7iISx\nUEKaXt1/S1Ocvr9r+cvS0uHkyff2cOUZo6JeSxAEDjWE91WrlWrW7W5Co1YyqSiZq0ou5Z2DH1O/\nLQ584l7wkTnx6DQqdlS24W3MRZ9Wj18Rvb1s6qg0tgT/9hwcj+AxYDIcPQHtrzfMoM/jxx8QsBxF\n6GVkZGROJGTx/g6wt30/Zq2ZrCMyuSPFG+DT2pVRAtzm6pBiqn2+PinmfaR4hzp7DRR/HSiD+8vy\n388OUtvq4IUl+yCYx9bU6uVQ7WH+9e5uDMGieZ9vr2XLF6INvHh9DaOHW6nfNCnqWjkpZlITYthR\n2UaRdhpnTbDyt+3/jHoPC6fmsGWH+Fjwie/RdAxBlvtZy8jIfB+Rxfs44wv4eHznMwA8PuehqOdc\nA7TsDLXmBKh3NGIIinmf340nWAhFp4wW71D8e0DL+38Qb39AdOl7/QECbj1KXR9/++8eQvujYlxZ\nOA11bN3lAkTrv97moN7Wv/LamLwECrPjmDwyGb1WTXNvS9TzWpWGVGvElraAuCAxG2RrWkZG5oeH\nHPM+ztj7ugZ9ru8IyxuQWm4CrGvYRCAYE+9299Du+v/27j2+qfr+H/jr5NY2TXolaaFyKeVSKCLW\ngrJCuQjqnO674cRuCiqKbrrNOXHD6tapa9GJt6nbUMd+juGsSre5ocO5DWVaAUELFJyC4w5t0nua\nS5Oc8/vjNKdJk0IKPW1DXs/HYw9y76cfmK+8P+dzkSd7BSrvKZny0rzAxiqJ2vDwPtUZ2tGSJMCz\nexZcO+YjeGFzy97JcH08D5JHDt2bv5wPg17+Jzc804iyJRcprx13XioEQUCiQf4+qdeEhnLP+wH9\ntfSLiCiWsPIeRJIkweayh9wPns3t8oeHd5O7RbkdfHxnu9eBqs/+DADK4SM3F1yP4x0nlK1PEyPs\nXnY24d3qkIfpXR4fIIb/U/L7BcDf/YUhx2LC8qsm49d/rsMNl03EuJxUfPniUUgzJ0CnDf0e2fMo\nzdyUUQCAB2f+GO/VHsPfIPdbfy39IiKKJQzvQSJKIl7c8wfU2vYoj7l8LhiDdjtz++TlYUZdEpxd\nQ+j1PWaLRxKovBN1CSF7lgcPmz8488c41HYUI0zRn6F+zN6BIw3tuGRyNiRJQmOb3CZ7a/iXjBsu\nm4A/vP1ZyGMjhhkxdkQKfn1PprIr2bXzxoW9Fwidwb54wtdQlDUNgLxmfLjJC8Ae8X1ERPGA4T1I\nPji+LSS4AXlI3Kg3YsuxGuxs2I3RXbuZpSWkKuEdMD2rENvrd0b87ARt5BnYSUET1oYlZfa6eUpv\nfvKifKKXJAJV/94Pl8cf8XXfuWYqLhqXiTf+8z+0OeXr8KYkffeQeBRHu5r0ybh67BUYkzIS+Rnj\nQ54zRjiDm4gonvC/goPkiON42GNtne3ITs7CK//9E4Duq8cmgwkI3XEURn0iknRJESe1GTSRwzsh\nwoS1M/HC3/ae8nmz0QCNIGDa+GF4r/YEym+ajqyM3rdg7c0VY+ZHfJxD5UQU7xjeg0QUw6vWNk/o\nubeBk8R0ETb60Gv0MOuT4fK5MMp8HmZkF+L1z98A0H3Nu6feKvLTqW9y4sO9obO/x2SbkZeTijST\nARve/SLkuRSj/HMWzxuHL00ZftoTuvpq4sg05FiSseCi058DT0R0LmJ4DzC/6MfWkzvR7pWXS5kN\nJrR3yreDZ5IDQHPXvuaBddrBDBo9kvRJgEsOZVPQsZ4913kHSJIUVRtFSYIAucKtO9iEx1/5JOw1\n2ZlGXL9wAtqdnWHhbe46ucuYqMeEkf1/XGaCQYuHb7m43z+XiChWMLwH2Oufv4H3jtUo939c9H00\ne1rx+I7n0NYZuv45cBKYNsLBIQatQdkuVJSkkGM9e6uwfVLka9Q9/eHtz7Dni0b85MYiPLthd8TX\nmLt2NjMbw3+W2WgAfL6ofhYREfUd13kPsODgBuTh7xSDCYBceft7DKcnahOUk7yCA1qv0Strn32i\nr8dzkcN7uDELAJCfPj7i8wDg9flRs+ck7K1u7DrQCI/Xj6l54RPbgvcUnzQ6PfS5ZG6cQkSkJlbe\nA6jJ3Rz2mFajgV6Sw9Yn+uDwhp6clahLxNyRszA8ORtOnwtr69YDkK9rB9ZCe0VvyLB5z4NJAiZm\njMMPLvw2Rppzem3jvkMt8Hj9Xbfl9k4YmYZdPQ4MSUnu/oJw1zemosXhwco1H8ptNugQegGAiIj6\nE8N7ADk6O8Ie0wo66DXytehPmz5H2fsPhzyfpEuERtBgUuYE7GvsXjdt0OiVYXOfFFp593bNGwDG\np48Ne+yDPSdQ3+RCUb4Vn+zvXj8dOJ7TmhY+Uzx4W1KDXgtrujHsNUREpA6Gt8pqjm/Hnw+8CUmS\nIp7gpRU0QFcId/jCz6sOrqiDN1nRa/XKOdpaQYuEoK1PNRGukffk8vjQ0OyCKEl48W/7AABvfngI\nflGCRhAgShJaunZQy4hwlGaka93zLsxBArcrJSJSHcNbZdvqP4bD2wGtoMUxx4mQ5wQI0AiaU65b\nDt6+NHh7U4PGgK+O/TJaPe34+rgro1r73NzuwX3P1+CGhRPx1tZDONHoVJZxzZo6HP/ZJbfvgnGZ\n+GS/HYHJ6enm8PXhkc7RXnL5xNO2gYiIzh4nrKlIkiQcaz8OS1ImLMZhYc9ru4JbI2h6rZZTEoLC\nWxtaeZsMyfjOBTcjO1meiHbJ8CLkGSfh4Mm2iJ+1dW89Or0i1r65Dyca5Sr/0Ml2nGcx4eYv56Pk\nguEA5CDPChoGT002oCjfGvJZkcKbiIgGBsNbRc2eFnT4nDjPNCLsmE4gdP/ungdxBPReeYeH5/X5\n12LP5tF46P99FPGzelvnPXFUGgRBwNLL83H/koswbdwwXFKQpTyv0QhYftUkPLRshvJYUgIHbYiI\nBgvDW0VH2+UtUM8z50Tc9UwndAegXjh9eAev3470eW5P99pqMUJQB2aR9zQuR74Wr9EIyMuRj+Ys\nuUA+RjQwZK7XaXGe1YQll0/E/83K5RalRESDiOWTCpxeJ/RaA052yCeA5Ziy8b/Wg2Gv02i6vzsF\nb4G6fMoSvLBnHYDQ8A4eWo+0ltsZFN5NbW4MSw2dJR6YgNZTXk5K2GNppgT89KYimBJDvyTMu7D3\nZWZERDQwGN79zOVz4d4tP8PkjInIMcnXkJP1ydBHWL4VXHkHD5sPT+4esu7tvO1IlbfT3R3eJxqd\nEcLbo9w2G/X48sWjcaTBgcwIs8kBYEx2eKgTEdHgY3j3M5tL3sxkb9N/kZmUAUCeaBbxmrcQXHkH\nB3l3MAdPWAumj3DN2xVUeb/yz8+ROzwFf/vgIP618yh+eddsNLd3h3eCXosrLh4V7a9FRERDCMO7\nnzW7W5Xbbp8clgnahIgbp2g1kSvv4NvB67yDRay8PaGV96Zth/H29iMAgGP2jpDKW6vldAciolil\nanhXVlaitrYWgiCgrKwMU6dOVZ5bv3493njjDWg0GkyZMgX333+/mk0ZMMFboHr8clgm6hIihm1I\n5R08eU2jw7Xj/w/NnpawJWTXTvg/HG0/HnFpWaDyHp5pxIlGJz4/0qI8d+BYG9qd3u6fp+GEMyKi\nWKVaeG/btg2HDh1CVVUVDhw4gLKyMlRVVQEAHA4Hfvvb3+Ltt9+GTqfDsmXL8Mknn2DatGlqNWfA\nNLqblNsunwuAPEs8UuWt62WpmF6jw9yRxRE/f+55kR8HAJdHnk3+tdlj8bcPDuLgye4dxl/55+ch\nr9UyvImIYpZqY6c1NTVYsGABACAvLw+tra1wOOQjL/V6PfR6PZxOJ3w+H1wuF1JTw7cOjRWOzg5s\nP/kxJElCo6u78m5yt0AnaKHT6CIe0xl8Trc+KLyD139HIkoSnvvTbrzz0ZGQx51uubI2JuiQl5OK\nTp8Y8nzRRAsqll+MEcOScctVk6P/BYmIaEhRrfK22+0oKChQ7mdkZMBms8FkMiEhIQF33nknFixY\ngISEBHzlK19Bbm6uWk1R3bp9r2JP4z74RF9I5d3oblIODDEETVgTIECC1GOTlu7bp9ub3OH0Ysd/\nbdjxXxsWFI1UHg9U3kkJOuSNSMHmj4+FvK/4/OEYnpmMn9968Rn8lkRENFQM2IS14N29HA4H1qxZ\ng7///e8wmUy48cYb8emnnyI/P7/X96enG6HT9e+hFxZL5JncfbWnUT7YY3dLHZo8ocd+Gg1JsFjM\nyHQE75SWAJfPjaQEg9IGY1L3cq3TtavD1739aWamCZquIXCxa+OU80akYuSIVPx2476Q95UUjYJB\nPzAHh/RX31Io9qt62LfqYL+qQ7XwtlqtsNu7j5dsaGiAxWIBABw4cAAjR45ERoa8lKqoqAh79uw5\nZXg3N4efuHU2LBYzbLazP3W6xdM9u7z25N6w5/XQw2Zrh8fZPYQdGC73+6C0QeyeSxaxXU1tbmz+\n5Di+WjwGB4Mmov3+b3vw7ifHUZCbgZNd+5W7OjxIMephStLD4fLia7NzcUlBNlpb+rcPe9NffUuh\n2K/qYd+qg/169nr78qNaeBcXF+OZZ55BaWkp6urqYLVaYTKZAAA5OTk4cOAA3G43EhMTsWfPHsyZ\nM0etpqhi9UfPwSt6cdRxPOy5zMQMZfg8cIzn6a5563rZHhUA3J0+PPlqLY7ZO5Bi1CM5aNezDe9+\nAQB495PudhgTtBAEAXkjUlB7oBFjslMinslNRESxSbXwLiwsREFBAUpLSyEIAsrLy1FdXQ2z2YyF\nCxfilltuwdKlS6HVanHhhReiqKhIrab0O0mS8L+2Q8p9kz4Zl4+ehw37/wYAyEsbg8aTcngHztmO\ndJCILji8teF/FbsONMLh6lTO2wYAh8sLnz903/J7S6dh52d2/HPnUQDyPuQAMHNKNo7ZOzAmm8NW\nRETnElWvea9YsSLkfvCweGlpKUpLS9X88arxS6EHfKycfhc6Ra8S3hPTx2HbyZ0Auo/xDF4qJkC+\nNh08YS3SwSRPvVYb9li70xsyi1ynFTBpTAZGZZuV8A6YMSkLMyZl9fwIIiKKcdxh7Qz4RF/I/fTE\nNIhSd6BOypio3A5U3qcdNu9xJKgoRj6+s6HFhRSj/Flfn52LWVPl07+SE/UoW3JRr+8jIqJzB8P7\nDPiCKu/ApikaQYMxKaNQ77QhNWg/8gRdeOXdVXj3CO/QWeDuzsjHd9qaXcrty2aMQkLQ7PHA0Z5E\nRHRuY3j3gcffiSZ3M5J08rKuqcMKcM34q5Xnf1j4HUiQK99knREdPqdSpRsiHEwSfCa20GNtt7sz\ntLrPy0nBcbsT9lY3/KKEBIM2JLiJiCh+8HSKPvhd3Xr8fOvj+KJVnqxm1CWFbKii1WiV4e/AaWCt\nHnlNdqRr3iGk0OHu4BPCAOD+JUWYnm+FKElobHPDkhr5GE8iIjr3sfLug912edb3580HAJx6G9O8\n1DE40VGvnMdt0OrxvWnLkZmYgSd2/qrrVXJgS5KEL060hbzfFTRs/s1LxwMAvjE3DxNHpUEUJeRx\niJyIKG4xvM9AoJruOcks2DfGfxWZSRmYnTNTeSw/Qw7hnnX3UVsHdn/RCH1O92OBYfNFJWOxcLq8\nBaopSY+ZBdn98BsQEVEs47B5HyTp5I1OWjoD4d175a3X6nHZ6HnK9fFIAiPlB4Oq7sCQurtrn/JE\nA69rExFRKIZ3HwQOGWnzyNv9nWpXtFMLrb0P1odvHxi45p2UwMERIiIKxWTog2S9EXZXI1qjqLwj\nqftfE+qbncohLQdPtuGnH2yFVqNR/iYClXfgmneigX9FREQUisnQB8auYfPAhiynuubdU7uzE49X\nfQIASJ3uBwTgcL0DXluH/FlB17ub2txwK5U3h82JiCgUw/ssBO9NfjqfH+0+fcwvSoAWgNC9PEzs\nkGePJzhHYsWvPsCE8+T7rLyJiKgnJkMf+IO2QAX6Vnl/FnSMpyhCDu8gVu1o1O9JgMslLy37rCvs\nWXkTEVFPnLDWB/4ee5r3Nby1GgGjskwQpfD9xy+aYIXkTAWk0L8SVt5ERNQTw7sPelbe2iiHzX1+\nEUcaHBiVZcaIYck9N1ODViNg8piMiO9l5U1ERD2xrOuDM628TzY64RcljLSakJpsADq6n5sxyYqR\nVhPSTBH2Pge4fzkREYVhePfBmV7zPtLgAACMtJqQoNeis/ZCGEbvxThdEZZfPRlajQZtHZ1h70sx\nGUIOLyEiIgIY3n3il0KP6Yx2nfcRmxze51mSMSw1CWNTR+KSnOmYX5ijhLMxMfyv4pqSvLNsMRER\nnYsY3n3gE3uEd5Q7rB3tqrzPs5qQnKjH/UuLwl6j02qQaNDC3elHokGL5VdNxoUTLGffaCIiOudw\nwlofhFfe0YW3rdUNU5IeyYn6U74uUH2PspoY3ERE1CuGdx+cybC5JElobnMjM+X052/rNPJfh/E0\nIU9ERPGN4d0H/p7D5r1U3j6/iM2fHENTmxsOlxedPhEZKQmn/XxBI1//1uv410JERL3jNe8+CKu8\ne1nn/bs3P0VN3UnMmTYCc6fJm5ZnRFF5d2V3xE1ciIiIAlji9UHYJi0RKu/j9g7U1J0EAHS4fWhq\ncwNAVMPmmq70FkWGNxER9Y7h3Qd+0Q+9pvt6dKRr3htrDnXfkSQ0doV3VMPmXceBsvAmIqJTYXhH\nSZRESJCUY0GByEvFPv7cpuyW5ur0o6nNAyDaylv+08/Km4iIToHhHaXAGu9kvVF5rGfl7fH64e70\nI8digk4rwOXxwd7qAhDdNe+vzRoLALhixsj+ajYREZ2DOGEtSoHJaknBlXePa97tTnmLU7NRj6QE\nHVweH056RSTotRH3Lu9p2vhhWLtyfj+2moiIzkWqhndlZSVqa2shCALKysowdepUAEB9fT1WrFih\nvO7IkSO45557cPXVV6vZnLMSCG+jvju8e54q1u70AgDMSQYkJejgdPvg6vQhK93IPcqJiKjfqBbe\n27Ztw6FDh1BVVYUDBw6grKwMVVVVAICsrCysW7cOAODz+bBkyRLMnz+0K87AGm9D0IS1noEcCO+U\nZD2SDDo0NMtD5lkZRhAREfUX1a5519TUYMGCBQCAvLw8tLa2wuFwhL3uT3/6Ey6//HIkJyer1ZSz\ndrKjHmXv/xwAoDnFGd7dw+aGkHO4s9KTensLERFRn6kW3na7Henp6cr9jIwM2Gy2sNe99tpr+MY3\nvqFWM/rF5qMfKLdPtSVq97C5fM07ICudlTcREfWfAZuwJkVYvPzxxx9j7NixMJlMp31/eroROl10\nR3BGy2IxR/U689HuyjnZmIgrJ8yH0+sKeX+n1w97u7wsbFROGtIPtyjP5Y8dFvXPOlfE2+87UNiv\n6mHfqoP9qg7VwttqtcJutyv3GxoaYLGEnpS1efNmzJw5M6rPa2529mv7LBYzbLb2qF7rdXfvrOb1\n+PH1CfLEuuD3/+Yve7BtXwMAwNfphSboy4pRJ0T9s84Ffelbih77VT3sW3WwX89eb19+VBs2Ly4u\nxqZNmwAAdXV1sFqtYRX27t27kZ+fr1YTztoxxwkcd5wMWRLWc4Y5ABy1OZTgBrpnmwcEjvokIiLq\nD6qlSmFhIQoKClBaWgpBEFBeXo7q6mqYzWYsXLgQAGCz2ZCZmalWE85a5bYnAQBXj71CeSxSeG/b\nVx9yPylBCwncJY2IiNShakkYvJYbQFiV/de//lXNH68KrUaLjz+zwePzo7nNg0lj0uHomqi2eN44\npCTrIQiCsi1qSvLpN2chIiLqC47nRsEnepXbOkGLZ6p3hzw/Y5IVAHBJQRbSTPIBJCUXjMAHe07i\nWwvGD1xDiYgoLjC8o+AVfd23feHD4R1u+Xlj0HXuCSPT8OKP5inHfBIREfUXHkzSi+ClbcHh3d7h\nC3vtF8fboNMK0OtCu5PBTUREaoiq8pYkKa725v5f62HsbKhV7gcPm7c4vABCz+Z2eXwwG/Vx1UdE\nRDR4oqq8582bhyeffBJHjhxRuz1Dwuodz+JfR7Yo94Mr72ZH5PXmwUPmREREaooqvF977TVYLBaU\nlZXh5ptvxl//+ld0dnaq3bYhIzi8XV55FvmtV03CbV+drDzOtdxERDRQogpvi8WCG264AevWrcPP\nfvYz/PGPf8Ts2bPx5JNPwuPxqN3GQef1dw+b+yU5yGcWZKNoolV5nJU3ERENlKgnrG3fvh333Xcf\nli9fjsLCQrz88stISUnBXXfdpWb7hoROf/cogw8+6HUaCIIAnVaDwFXupER95DcTERH1s6jKxYUL\nFyInJweLFy/GQw89BL1eDqq8vDy88847qjZwKHD7u0cXJJ8Wem33dx6DXguP1w9jQv8emkJERNSb\nqML7xRdfhCRJGDNmDABg7969mDxZvt778ssvq9a4ocLTVXnPHzkb2w+kQgxaEpag18Dj9QPgTHMi\nIhoYUQ2bV1dXY82aNcr9559/HqtXrwaAuFge5fF7YNQl4ZrxV8Pn1Yas5zbo5Yq70+sfrOYREVGc\niSq8t27dilWrVin3n3rqKezYsUO1Rg01Hr8H+q6Txbw+MSS8E7rC28PwJiKiARJVeHu93pClYR0d\nHfD5wncaO1d5/J3QaeTr/F6fGHLN+5KCLADAlNyMQWkbERHFn6iueZeWluLKK6/ElClTIIoidu/e\nje9+97tqt23IECWx18r7yktGY/KYDIzJjnxgOhERUX+LKryvvfZaFBcXY/fu3RAEAffddx9MJpPa\nbRtS9BodRFGCX5RCwlsQBOQOTxnElhERUbyJep230+lERkYG0tPT8cUXX2Dx4sVqtmvI0Wn08PpF\n+baO57kQEdHgiary/vnPf473338fdrsdo0aNwpEjR7Bs2TK12zak6DU6eH1yeAdf8yYiIhpoUaXQ\n7t278dZbbyE/Px8bNmzA2rVr4XK51G7bkKLTBoU3K28iIhpEUaWQwWAAIM86lyQJU6ZMwc6dO1Vt\n2FCj1+jh8zO8iYho8EU1bJ6bm4v169ejqKgIN998M3Jzc9He3q5224aUkGFzHbdCJSKiwRNVeD/4\n4INobW1FSkoKNm7ciMbGRtx+++1qt21I0fGaNxERDRFRhXdlZSXuv/9+AMDVV1+taoOGKn3QbHMO\nmxMR0WCKKoW0Wi1qamrg8XggiqLyv3ORJEkRHw8dNmd4ExHR4Imq8n7ttdfw0ksvhQSbIAjYt2+f\nag0bLKIU+UuJjuFNRERDRFThHU+HkPikyAeMiH4Br723HwCveRMR0eCKKryffvrpiI/fdddd/dqY\nocAvRg7vT/7bhGN2eckcK28iIhpMUV/zDvxPFEVs3br1nFoqJkkSNnz+V+y274W/l8rb5e6+ZMDw\nJiKiwRRV5d3zBDG/34/vfe97p31fZWUlamtrIQgCysrKMHXqVOW5EydO4Ic//CG8Xi8mT56Mhx56\nqI9N7z8tnlb868gW/OvIFlQU3x/xNXqtvvs2w5uIiAbRGaWQz+fD4cOHT/mabdu24dChQ6iqqkJF\nRQUqKipCnn/kkUewbNkyvP7669BqtTh+/PiZNKVfBE9S623Y3BAc3rzmTUREgyiqynvOnDkQBEG5\n39raiq9//eunfE9NTQ0WLFgAAMjLy0NrayscDgdMJhNEUcSOHTvwxBNPAADKy8vPtP39olP0Krd7\nm7AmiN27qrHyJiKiwRRVeL/88svKbUEQYDKZkJJy6jOs7XY7CgoKlPsZGRmw2WwwmUxoampCcnIy\nVq1ahbq6OhQVFeGee+455eelpxuh6+dtSS0WMwCgTdukPJaalhjxtX6x+8vLsGEm5b0UGftHHexX\n9bBv1cF+VUdU4e1yufCXv/xFCdj77rsPy5Ytw/jx46P+QcFrxCVJQn19PZYuXYqcnBzcdttt2Lx5\nM+bOndvr+5ubnVH/rGhYLGbYbPKku/rmZuXxentLxNc7nd1D641NHbClJPRre84lwX1L/Yf9qh72\nrTrYr2evty8/UY3/Pvjgg5gzZ45y/5prrjntBDOr1Qq73a7cb2hogMViAQCkp6djxIgRGDVqFLRa\nLWbOnInPP/88mqaoInjYvK0z8j80d9Bsc51GiPgaIiKigRBVePv9fhQVFSn3i4qKet1GNKC4uBib\nNm0CANTV1cFqtcJkMgEAdDodRo4ciYMHDyrP5+bmnkn7+0WnPyi8PXJ4ZxutGJs6Wnnc7ZGQZjJg\n6eUTMWFk2oC3kYiIKCCqYXOz2YyXX34ZF198MURRxJYtW5CcnHzK9xQWFqKgoAClpaUQBAHl5eWo\nrq6G2WzGwoULUVZWhpUrV0KSJEyYMAHz58/vl1/oTHT6O5XbrZ1tAIBC61SMTR2DZ2tfBACIfg1G\nDjdj7oU5g9JGIiKigKjCe9WqVXj88cfxxz/+EYAczKtWrTrt+1asWBFyPz8/X7k9evRo5fMGW/Cw\neWvXsLlWo4VWEzQwIWqQnBhVdxEREakqqjTKyMjA8uXLMWbMGADA3r17kZGRoWa7BpQ3qPJu7xo2\n1wpaiEEzzCFpYGR4ExHREBDVNe8nn3wSa9asUe4///zzWL16tWqNGmgef+TK29vZPcNcErUwJurD\n3ktERDTQogrvrVu3hgyTP/XUU+fUSWOdYnflfbKjAYBceXs6gyblcdiciIiGiKjC2+v1orOzO+A6\nOjrg8/lUa9RAJAKtFQAAGGJJREFU8wZV3m6/GwCgFTRwB1XeHDYnIqKhIqo0Ki0txZVXXokpU6ZA\nFEXs3r0bN954o9ptGzCByvuCYQWotdcBALQaHZxdlbckAZAEGBM4bE5ERIMvqvC+9tprMWbMGDQ3\nN0MQBMyfPx9r1qzBTTfdpHLzBkZgnXehdWp3eAsauDxd+5yLWgACh82JiGhIiCqNKioq8J///Ad2\nux2jRo3CkSNHsGzZMrXbNmACS8VygzZl0QpauD1dw+aSfHWBw+ZERDQURHXNe9euXXjrrbeQn5+P\nDRs2YO3atXC5XGq3bcAENmlJMXTvIavVaOFyd4W3KHdTMmebExHREBBVeBsMBgDyxDVJkjBlyhTs\n3LlT1YYNpE5/JwQI0Gm6K+v2znYlvCWRlTcREQ0dUaVRbm4u1q9fj6KiItx8883Izc1Fe/u5cVJM\ne6cDJzsaoNfqIQgCvjR8Bj44sQ1ZRit2upuABACiFhpBQKKhf48kJSIiOhNRhfeDDz6I1tZWpKSk\nYOPGjWhsbMTtt9+udtsGxOM7nkOHzwmDRh4S/2b+IswbOQu1dR7U7m9C0kVQlokJAk8TIyKiwRdV\neAuCgLQ0+SStq6++WtUGDTSbqxFA96Q1jaDBCFM2Hvj3v4Cuoz8lbtBCRERDSFTXvM9lek14KIuB\n406lrkpb5AYtREQ0dMR9eKcnyiMKSyddpzzW0u4BAORmpyE/sQi+htHc15yIiIaMuA9vvygiLSEV\nFw+/SHmsoVleBjd5TAauHH05xOYsDpsTEdGQwfCW/NAKobPIG1rk8LamJSEjJRFajQBLWtJgNI+I\niChM3JeToiQqM80DApW3NT0J6eYE/GzZDAxLSRyM5hEREYWJ+/D2S35oNKGVd1ObfLJYZqoc2DnD\nkge8XURERL2J+2FzURKhFUK7obVD3i41NTlhMJpERER0SnEf3n7RHzG8kxN10OvivnuIiGgIivt0\n8kti2IS1VocHqSZW3URENDTFfXiLkghNUOXt9YnocPuQmmwYxFYRERH1Lq7DW5RESJBCKu92Z+B6\nN8ObiIiGprgOb78kH/mpDZptHpislsLwJiKiISq+w1v0A0DIsHmro6vyNjG8iYhoaIrr8BYlObyD\nh83bOGxORERDnKqbtFRWVqK2thaCIKCsrAxTp05Vnps/fz6ys7Oh1crBuXr1amRlZanZnDCBYfPQ\nyls+lITD5kRENFSpFt7btm3DoUOHUFVVhQMHDqCsrAxVVVUhr3nhhReQnDx4u5f5lcpbA1GSsLHm\nEA4cbwMAmJMY3kRENDSpFt41NTVYsGABACAvLw+tra1wOBwwmUxq/cg+E4MmrO38rw1/eu8L5Tmz\nkUeAEhHR0KTaNW+73Y709HTlfkZGBmw2W8hrysvL8c1vfhOrV6+GJElqNaVXfrF72Nzd6Q95juFN\nRERD1YAdTNIznL///e9j9uzZSE1NxZ133olNmzbhiiuu6PX96elG6HTaXp8/E6np8sEjpqREpCd3\nH/mZlKDFiOFp/fqz4o3FYh7sJpyT2K/qYd+qg/2qDtXC22q1wm63K/cbGhpgsViU+1/72teU2yUl\nJfjss89OGd7Nzc5+bZ/FYoa9sR0A0Onxo6HDoTyXnKiHzdberz8vnlgsZvafCtiv6mHfqoP9evZ6\n+/Kj2rB5cXExNm3aBACoq6uD1WpVrne3t7fjlltuQWenvCxr+/btGD9+vFpN6ZU/aKmY0+NTHueQ\nORERDWWqVd6FhYUoKChAaWkpBEFAeXk5qqurYTabsXDhQpSUlOC6665DQkICJk+efMqqWy2BCWsa\njQZOd3d467RxvfydiIiGOFWvea9YsSLkfn5+vnL7xhtvxI033qjmjz+t4MrbEVR5+8WBnzxHREQU\nrbguMQPbo2oFTciwOcObiIiGsvgO76Ad1oKHzUWGNxERDWEMb4RPWFs8f9xgNYmIiOi0Bmyd91Ck\nHEyi0cLp9sFs1OPp788e5FYRERGdGitvyMPmLo8PxoS4/i5DREQxIr7DWwxd521MZHgTEdHQF9fh\nHVjnLYmA1yey8iYiopgQ1+EdWOft6zqTJCmRO6sREdHQF9fhHai8OzvlP1ONPMObiIiGvrgO70Dl\n3emV73NPcyIiigXxHd5d53l7PPKf5mRW3kRENPTFdXgH1nm7u8I7hZU3ERHFgLgO78A6b3fXNW8z\nr3kTEVEMYHgDcLm7Km8OmxMRUQyI8/CWh82V8OawORERxYC4Dm9RDIS3H1qNgCRu0kJERDEgrsM7\nMGzudPthNuohCMIgt4iIiOj04jy85crb6fLDlMTr3UREFBviOrwDO6x5vBIPJSEiopgR1+EdGDaH\nJPBQEiIiihnxHd5dE9YgaZCUoB3cxhAREUUprsNbDKq8OdOciIhiRVyHd2DCGsObiIhiSVyHt1f0\nAQAkScNr3kREFDPiOrxPdtRDLxgAr4GVNxERxYy4DW+PrxP1ThtSNcMAcNiciIhiR9yG95HW45Ag\nIRmZAMDwJiKimKFqeFdWVuK6665DaWkpdu3aFfE1jz/+OJYsWaJmMyI62HIEAJAkZgAAr3kTEVHM\nUC28t23bhkOHDqGqqgoVFRWoqKgIe83+/fuxfft2tZpwSg0djQAAjdcEAEjkOm8iIooRqoV3TU0N\nFixYAADIy8tDa2srHA5HyGseeeQR3H333Wo14ZQCG7R4O+X7rLyJiChWqBbedrsd6enpyv2MjAzY\nbDblfnV1NWbMmIGcnBy1mnBKga1R3Z3yn7zmTUREsWLAEkuSJOV2S0sLqqur8bvf/Q719fVRvT89\n3Qidrv+GtsXDcmiLogBBkHDeiDRoNDwStL9YLObBbsI5if2qHvatOtiv6lAtvK1WK+x2u3K/oaEB\nFosFAPDhhx+iqakJ119/PTo7O3H48GFUVlairKys189rbnb2a/sCu6t1OL1INCSgsdFxmndQtCwW\nM2y29sFuxjmH/aoe9q062K9nr7cvP6oNmxcXF2PTpk0AgLq6OlitVphM8uSwK664Am+++SZeffVV\nPPvssygoKDhlcKtBGTb3iDByshoREcUQ1SrvwsJCFBQUoLS0FIIgoLy8HNXV1TCbzVi4cKFaPzZq\nohgIbwmZSbzeTUREsUPV1FqxYkXI/fz8/LDXnHfeeVi3bp2azYgoMGzu9ohISmN4ExFR7IjbHdYC\nS8Ukbo1KREQxJn7DO+gsb67xJiKiWMLw5lneREQUY+I2vMWuYXOGNxERxZq4De/QyptLxYiIKHbE\nbXiLUnflzWveREQUS+I2vP1d67zB2eZERBRj4je8JRECNAAEJDK8iYgohsRteIuiHwLkg0g4bE5E\nRLEkbsPbJ/kBSf71OWxORESxJG7DWxRFpfLmbHMiIoolcRvefkkEJA6bExFR7Inj8PZD6grvRAPD\nm4iIYkfchrcoypV3okELjUYY7OYQERFFLW7D2y+JkESu8SYiotgTx+HthyTxejcREcWeuA1vURQh\nsvImIqIYFLfh7RP9PFGMiIhiUtyGt7xUTMM13kREFHPiN7y7Km9e8yYiolgTt+EtSiIkDpsTEVEM\nitvw9ku85k1ERLEpLsNblHiWNxERxa74Dm9J4IQ1IiKKOQxvVt5ERBRj4jK8/UHhzdnmREQUa+Iy\nvLsrbw0Meg6bExFRbFG17KysrERtbS0EQUBZWRmmTp2qPPfqq6/i9ddfh0ajQX5+PsrLyyEIA3O6\nVyC8JUmAXheX31+IiCiGqZZc27Ztw6FDh1BVVYWKigpUVFQoz7lcLmzcuBHr16/HK6+8gi+++AIf\nf/yxWk0J45f88g2GNxERxSDVkqumpgYLFiwAAOTl5aG1tRUOhwMAkJSUhJdeegl6vR4ulwsOhwMW\ni0WtpoTxi91LxQw6DpsTEVFsUW3Y3G63o6CgQLmfkZEBm80Gk8mkPPb888/j97//PZYuXYqRI0ee\n8vPS043Q9VPQ+h1u+YYkIDsrBSnJhn75XOpmsZgHuwnnJPareti36mC/qmPAplpLkhT22G233Yal\nS5di+fLluOiii3DRRRf1+v7mZme/tcXe0dbVKAFtLU54nJ5++2yS/89qs7UPdjPOOexX9bBv1cF+\nPXu9fflRbdjcarXCbrcr9xsaGpSh8ZaWFmzfvh0AkJiYiJKSEuzcuVOtpoTxB8025zVvIiKK1ubN\n/4zqdU8//TiOHz+mWjtUS67i4mJs2rQJAFBXVwer1aoMmft8PqxcuRIdHR0AgN27dyM3N1etpoQJ\nzDYXIECjGZgZ7kREFNtOnDiOd97ZFNVr77rrHowYkaNaW1QbNi8sLERBQQFKS0shCALKy8tRXV0N\ns9mMhQsX4s4778TSpUuh0+kwceJEXHrppWo1JUwgvDUCq24iIorOE088in376jB79nRcdtmXceLE\ncTz11K+watVDsNka4HK5sGzZbSguno3vfvc2/PCHP8K///1PdHQ4cPjwIRw7dhTf//49mDmz+Kzb\nouo17xUrVoTcz8/PV24vWrQIixYtUvPH9yowbK5leBMRxaRX/7Uf2z9t6NfPnJ5vxeL543p9/pvf\nXILq6leRm5uHw4cP4le/ehHNzU2YMeMSfPnLV+HYsaP4yU9Worh4dsj7GhrqsXr1L/Hhhx/gL3/Z\nMPTDe6hi5U1ERGdj0iR5NZXZnIJ9++rwxhvVEAQN2tpaw147deo0APJcsMCS6bMVl+Ed2KRFo+Ea\nbyKiWLR4/rhTVslq0+v1AIB//OPvaGtrw3PPvYi2tjbceuuSsNdqtd1ZE2nl1ZmIy9IzUHnrWHkT\nEVGUNBoN/H5/yGMtLS0YPnwENBoN3n33X/B6vQPTlgH5KUOMcs2blTcREUVp9Ohc/Pe/n6Kjo3vo\ne+7c+fjggy24667vICkpCVarFb/73Quqt0WQ+quGV1l/LvTfZduLNbv/H9LapqHia9/qt88lGTdm\nUAf7VT3sW3WwX8/egG/SMpR1+nwAAK0mLn99IiKKcXGZXp1d1yx0HDYnIqIYFJfh7WXlTUREMSwu\n08vrl8Nbr4nLlXJERBTj4jS85WFzVt5ERBSL4jK9Mg1ZEJ0mpGmtg90UIiKiPovL8E7TWuDZMwtp\numGD3RQiIjrHfOMbV8PpdKr6M+IyvL1+eZMWnuVNRESxKC5nbHl9DG8iIuqbZcuuR2Xl48jOzsbJ\nkydw3333wGKxwuVywe124+6778XkyVMGpC1xHd4GHdd5ExHFour9f8PHDbv79TMvtJ6PReOu6vX5\nkpJ5eP/993DNNYuxZcu7KCmZh7y88SgpmYsdO7Zj/fqXUFHxWL+2qTdxWXp6ffJsc1beREQULTm8\ntwAA/vOfdzFr1hy8++4/8Z3v3IJf//oZtLaGHweqlrisvDNTEmHQaTA80zjYTSEiojOwaNxVp6yS\n1TB2bB4aG22orz+J9vZ2bNmyGcOGWfGTnzyMTz/di2effWrA2hKX4T1xVDqqKr+C5qaOwW4KERHF\nkJkzZ+H553+F2bPnoKWlGXl54wEA7777b/i6du8cCHE7bqzTxu2vTkREZ2jOnHl4551NmDv3Ulxx\nxVdQVbUed999JwoKpqCxsREbN74xIO2IyyNBAR5Vpyb2rTrYr+ph36qD/Xr2eCQoERHROYLhTURE\nFGMY3kRERDGG4U1ERBRjGN5EREQxhuFNREQUYxjeREREMUbVHdYqKytRW1sLQRBQVlaGqVOnKs99\n+OGHeOKJJ6DRaJCbm4uKigpoNPwuQUREdDqqpeW2bdtw6NAhVFVVoaKiAhUVFSHP//SnP8Uvf/lL\nvPLKK+jo6MCWLVvUagoREdE5RbXwrqmpwYIFCwAAeXl5aG1thcPhUJ6vrq5GdnY2ACAjIwPNzc1q\nNYWIiOicotqwud1uR0FBgXI/IyMDNpsNJpMJAJQ/Gxoa8P777+Ouu+465ef1tkXc2VDjM0nGvlUH\n+1U97Ft1sF/VMWAXmSNtod7Y2Ihvf/vbKC8vR3p6+kA1hYiIKKapFt5WqxV2u12539DQAIvFotx3\nOBxYvnw5fvCDH2DWrFlqNYOIiOico1p4FxcXY9OmTQCAuro6WK1WZagcAB555BHceOONKCkpUasJ\nRERE5yRVjwRdvXo1PvroIwiCgPLycuzduxdmsxmzZs3C9OnTceGFFyqvveqqq3Ddddep1RQiIqJz\nRsyc501EREQy7opCREQUY1TdYW2oOtXObxSdzz77DHfccQduuukm3HDDDThx4gR+9KMfwe/3w2Kx\n4LHHHoPBYMAbb7yBl156CRqNBosXL8a111472E0f0n7xi19gx44d8Pl8uP3223H++eezX8+Sy+XC\nypUr0djYCI/HgzvuuAP5+fns137kdrtx1VVX4Y477sDMmTPZtwNBijNbt26VbrvtNkmSJGn//v3S\n4sWLB7lFsaejo0O64YYbpAceeEBat26dJEmStHLlSunNN9+UJEmSHn/8cWn9+vVSR0eHdNlll0lt\nbW2Sy+WSvvKVr0jNzc2D2fQhraamRrr11lslSZKkpqYmac6cOezXfrBx40bp+eeflyRJko4ePSpd\ndtll7Nd+9sQTT0iLFi2SNmzYwL4dIHE3bH66nd/o9AwGA1544QVYrVblsa1bt+LSSy8FAMybNw81\nNTWora3F+eefD7PZjMTERBQWFmLnzp2D1ewhb/r06Xj66acBACkpKXC5XOzXfnDllVdi+fLlAIAT\nJ04gKyuL/dqPDhw4gP3792Pu3LkA+N+CgRJ34W2320M2hAns/EbR0+l0SExMDHnM5XLBYDAAADIz\nM2Gz2WC325GRkaG8hn19alqtFkajEQDw+uuvo6SkhP3aj0pLS7FixQqUlZWxX/vRo48+ipUrVyr3\n2bcDIy6veQeTONm+3/XWp+zr6Lzzzjt4/fXXsXbtWlx22WXK4+zXs/PKK69g3759uPfee0P6jP16\n5v785z9j2rRpGDlyZMTn2bfqibvwPt3Ob3RmjEYj3G43EhMTUV9fD6vVGrGvp02bNoitHPq2bNmC\n3/zmN3jxxRdhNpvZr/1gz549yMzMxPDhwzFp0iT4/X4kJyezX/vB5s2bceTIEWzevBknT56EwWDg\nv9kBEnfD5qfb+Y3OzJe+9CWlX99++23Mnj0bF1xwAXbv3o22tjZ0dHRg586dKCoqGuSWDl3t7e34\nxS9+gTVr1iAtLQ0A+7U/fPTRR1i7di0A+bKZ0+lkv/aTp556Chs2bMCrr76Ka6+9FnfccQf7doDE\n5SYtPXd+y8/PH+wmxZQ9e/bg0UcfxbFjx6DT6ZCVlYXVq1dj5cqV8Hg8GDFiBFatWgW9Xo+///3v\n+O1vfwtBEHDDDTfgq1/96mA3f8iqqqrCM888g9zcXOWxRx55BA888AD79Sy43W7cf//9OHHiBNxu\nN7773e9iypQp+PGPf8x+7UfPPPMMcnJyMGvWLPbtAIjL8CYiIoplcTdsTkREFOsY3kRERDGG4U1E\nRBRjGN5EREQxhuFNREQUYxjeRHTWqqursWLFisFuBlHcYHgTERHFmLjbHpUonq1btw5vvfUW/H4/\nxo4di1tvvRW33347SkpK8OmnnwIAnnzySWRlZWHz5s147rnnkJiYiKSkJDz88MPIyspCbW0tKisr\nodfrkZqaikcffRQA4HA4sGLFChw4cAAjRozAs88+C0EQBvPXJTpnsfImihO7du3CP/7xD6xfvx5V\nVVUwm8344IMPcOTIESxatAgvv/wyZsyYgbVr18LlcuGBBx7AM888g3Xr1qGkpARPPfUUAODee+/F\nww8/jD/84Q+YPn063n33XQDA/v378fDDD6O6uhqff/456urqBvPXJTqnsfImihNbt27F4cOHsXTp\nUgCA0+lEfX090tLSMGXKFABAYWEhXnrpJRw8eBCZmZnIzs4GAMyYMQOvvPIKmpqa0NbWhgkTJgAA\nbrrpJgDyNe/zzz8fSUlJAICsrCy0t7cP8G9IFD8Y3kRxwmAwYP78+fjpT3+qPHb06FEsWrRIuS9J\nEgRBCBvuDn68tx2VtVpt2HuISB0cNieKE4WFhXjvvffQ0dEBAFi/fj1sNhtaW1uxd+9eAMDOnTsx\nceJEjBkzBo2NjTh+/DgAoKamBhdccAHS09ORlpaGXbt2AQDWrl2L9evXD84vRBTHWHkTxYnzzz8f\n119/PZYsWYKEhARYrVZcfPHFyMrKQnV1NR555BFIkoQnnngCiYmJqKiowN13362c0VxRUQEAeOyx\nx1BZWQmdTgez2YzHHnsMb7/99iD/dkTxhaeKEcWxo0eP4lvf+hbee++9wW4KEfUBh82JiIhiDCtv\nIiKiGMPKm4iIKMYwvImIiGIMw5uIiCjGMLyJiIhiDMObiIgoxjC8iYiIYsz/B9rFrWw98j7tAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "毕设（神经网络正式训练1）.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
